{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AQWA assistant functions\n",
    "\n",
    "# this function add new data to existing partitions\n",
    "# @newdata: the data to append, should already be pruned to have the same dimensions as the kdnodes\n",
    "# @kdnodes[i][0/1][k][0/1]: i partitions, 0/1 boundary/count  k dimension  0/1 min/max\n",
    "# return: the new kdnodes (with the number of records inside updated)\n",
    "def AppendData2Partitions(newdata, kdnodes):\n",
    "    \n",
    "    dims = newdata.shape[1]\n",
    "    for i in range(len(newdata)):\n",
    "        # find the kdnodes the data belongs to, this could be improved by using an index\n",
    "        for j in range(len(kdnodes)):\n",
    "            inside_tag = True\n",
    "            # check inside for each dimension\n",
    "            for k in range(dims):    \n",
    "                # an intersection holds if it intersecs in all dimensions\n",
    "                if newdata[i,k] > kdnodes[j][0][k][1] or newdata[i,k] < kdnodes[j][0][k][0]:\n",
    "                    inside_tag = False\n",
    "                    break\n",
    "            if inside_tag:\n",
    "                kdnodes[j][1] += 1\n",
    "                break\n",
    "    \n",
    "    return kdnodes\n",
    "\n",
    "# this function is used to generate a histogram for dataset\n",
    "# @dataset: numpy format, already pruned to the query dimension\n",
    "# @domain cloud be inclusive at both end\n",
    "# return: the histogram\n",
    "def CreateHist(dataset, m, n, domain):\n",
    "    \n",
    "    dim1_step = (domain[0][1]-domain[0][0])/m\n",
    "    dim2_step = (domain[1][1]-domain[1][0])/n\n",
    "    \n",
    "    hist = []\n",
    "    for i in range(m):\n",
    "        hist.append([])\n",
    "        for j in range(n):\n",
    "            hist[i].append(0)\n",
    "    \n",
    "    # find each dataset where it belongs to\n",
    "    for i in range(len(dataset)):\n",
    "        dim1_index = int((dataset[i,0] - domain[0][0]) / dim1_step)\n",
    "        dim2_index = int((dataset[i,1] - domain[1][0]) / dim2_step)\n",
    "        if dim1_index >= m:\n",
    "            dim1_index = m-1\n",
    "        if dim2_index >= n:\n",
    "            dim2_index = n-1\n",
    "        hist[dim1_index][dim2_index] += 1\n",
    "    \n",
    "    return hist\n",
    "\n",
    "# construct the accumulated histogram for dataset\n",
    "# return: the accumulation histopgram\n",
    "def CreatePrefixSumHist(hist):\n",
    "    \n",
    "    # in case it's not in np\n",
    "    hist = np.array(hist)\n",
    "    \n",
    "    # accumulation in second dimension\n",
    "    for i in range(hist.shape[0]):\n",
    "        for j in range(1,hist.shape[1]):\n",
    "            hist[i,j] += hist[i,j-1]\n",
    "    \n",
    "    # now do the accumulation in first dimension\n",
    "    for j in range(hist.shape[1]):\n",
    "        for i in range(1,hist.shape[0]):\n",
    "            hist[i,j] += hist[i-1,j]\n",
    "            \n",
    "    return hist\n",
    "\n",
    "\n",
    "def GetEmptyEularHist(m, n):\n",
    "    hist = []\n",
    "    for i in range(m):\n",
    "        hist.append([])\n",
    "        for j in range(n):\n",
    "            hist[i].append([0,0,0,0])\n",
    "    return hist\n",
    "    \n",
    "    \n",
    "# this create the Euler Histogram creation\n",
    "# C1, C2, C3, C4 = overlap, overlap but not in left/down/left and down\n",
    "def CreateEulerHist(queryset, m, n, domain):\n",
    "    \n",
    "    dim1_step = (domain[0][1]-domain[0][0])/m\n",
    "    dim2_step = (domain[1][1]-domain[1][0])/n\n",
    "    \n",
    "    hist = []\n",
    "    for i in range(m):\n",
    "        hist.append([])\n",
    "        for j in range(n):\n",
    "            hist[i].append([0,0,0,0])\n",
    "    \n",
    "    # so that we can add 1 for all elements\n",
    "    hist = np.array(hist)\n",
    "    \n",
    "    # for each query\n",
    "    for i in range(len(queryset)):\n",
    "        \n",
    "        dim1_index_min = int((queryset[i][0][0] - domain[0][0]) / dim1_step)\n",
    "        dim1_index_max = int((queryset[i][0][1] - domain[0][0]) / dim1_step)\n",
    "        dim2_index_min = int((queryset[i][1][0] - domain[1][0]) / dim2_step)\n",
    "        dim2_index_max = int((queryset[i][1][1] - domain[1][0]) / dim2_step)\n",
    "        \n",
    "        if dim1_index_min >= m:\n",
    "            dim1_index_min = m-1\n",
    "        if dim1_index_max >= m:\n",
    "            dim1_index_max = m-1\n",
    "        if dim2_index_min >= n:\n",
    "            dim2_index_min = n-1\n",
    "        if dim2_index_max >= n:\n",
    "            dim2_index_max = n-1\n",
    "        \n",
    "        for m in range(dim1_index_min, dim1_index_max+1):\n",
    "            for n in range(dim2_index_min, dim2_index_max+1):\n",
    "                hist[m][n][0] += 1\n",
    "                \n",
    "                # check if it overlap the left (i.e., dim2_min < n)\n",
    "                if n == dim2_index_min:\n",
    "                    hist[m][n][1] += 1\n",
    "                \n",
    "                # check if it overlap the down (i.e., dim1_min < m)\n",
    "                if m == dim1_index_min:\n",
    "                    hist[m][n][2] += 1\n",
    "                    \n",
    "                if m == dim1_index_min and n == dim2_index_min:\n",
    "                    hist[m][n][3] += 1\n",
    "    \n",
    "    return hist\n",
    "\n",
    "# construct the accumulated Euler Histogram for queryset\n",
    "# C1: do not aggregate this\n",
    "# C2: aggregate in dim2\n",
    "# C3: aggregate in dim1\n",
    "# C4: aggregate in dim1 and dim2\n",
    "def CreateAccumulateEulerHist(hist):\n",
    "    \n",
    "    # in case it's not in np\n",
    "    hist = np.array(hist)\n",
    "    \n",
    "    # accumulation in second dimension\n",
    "    for i in range(hist.shape[0]):\n",
    "        for j in range(1,hist.shape[1]):\n",
    "            hist[i,j,3] += hist[i,j-1,3]\n",
    "            hist[i,j,1] += hist[i,j-1,1]\n",
    "    \n",
    "    # now do the accumulation in first dimension\n",
    "    for j in range(hist.shape[1]):\n",
    "        for i in range(1,hist.shape[0]):\n",
    "            hist[i,j,3] += hist[i-1,j,3]\n",
    "            hist[i,j,2] += hist[i-1,j,2]\n",
    "    \n",
    "    return hist\n",
    "\n",
    "\n",
    "# SingleQuery[k][0/1]:  k dimension  0/1 min/max\n",
    "# hist: the existing EularHistogtram(non-accumulated)\n",
    "def InserQueryIntoEulerHist(SingleQuery, hist, accuhist, domain, m, n):\n",
    "    \n",
    "    # insert the query to Euler Hist\n",
    "    \n",
    "    dim1_step = (domain[0][1]-domain[0][0])/m\n",
    "    dim2_step = (domain[1][1]-domain[1][0])/n\n",
    "    \n",
    "    dim1_index_min = int((SingleQuery[0][0] - domain[0][0]) / dim1_step)\n",
    "    dim1_index_max = int((SingleQuery[0][1] - domain[0][0]) / dim1_step)\n",
    "    dim2_index_min = int((SingleQuery[1][0] - domain[1][0]) / dim1_step)\n",
    "    dim2_index_max = int((SingleQuery[1][1] - domain[1][0]) / dim1_step)\n",
    "    \n",
    "    if dim1_index_min >= m:\n",
    "        dim1_index_min = m-1\n",
    "    if dim1_index_max >= m:\n",
    "        dim1_index_max = m-1\n",
    "    if dim2_index_min >= n:\n",
    "        dim2_index_min = n-1\n",
    "    if dim2_index_max >= n:\n",
    "        dim2_index_max = n-1\n",
    "            \n",
    "    for m in range(dim1_index_min, dim1_index_max+1):\n",
    "        for n in range(dim2_index_min, dim2_index_max+1):\n",
    "            hist[m][n][0] += 1\n",
    "\n",
    "            # check if it overlap the left (i.e., dim2_min < n)\n",
    "            if n == dim1_index_min:\n",
    "                hist[m][n][1] += 1\n",
    "\n",
    "            # check if it overlap the down (i.e., dim1_min < m)\n",
    "            if m == dim2_index_min:\n",
    "                hist[m][n][2] += 1\n",
    "\n",
    "            if m == dim1_index_min and n == dim2_index_min:\n",
    "                hist[m][n][3] += 1\n",
    "    \n",
    "    # update the accumulated EulerHist\n",
    "    #accuhist = CreateAccumulateEulerHist(hist) # see if using the following code could be faster\n",
    "    \n",
    "    # 1. update C1\n",
    "    for m in range(dim1_index_min, dim1_index_max+1):\n",
    "        for n in range(dim2_index_min, dim2_index_max+1):\n",
    "            accuhist[m][n][0] += 1\n",
    "    \n",
    "    # 2. update C2, accumulate horizontal\n",
    "    for m in range(dim1_index_min, dim1_index_max+1):\n",
    "        for n in range(dim2_index_min, len(accuhist[0])):\n",
    "            accuhist[m][n][1] += 1\n",
    "            \n",
    "    \n",
    "    # 3. update C3, accumulate vertical\n",
    "    for n in range(dim2_index_min, dim2_index_max+1):\n",
    "        for m in range(dim1_index_min, len(accuhist)):\n",
    "            accuhist[m][n][2] += 1\n",
    "    \n",
    "    # 4. update C4, accumulate first horizontal, then vertical\n",
    "    for m in range(dim1_index_min, len(accuhist)):\n",
    "        for n in range(dim2_index_min, len(accuhist[0])):\n",
    "            accuhist[m][n][3] += 1\n",
    "        \n",
    "#     return hist, accuhist   # the argument is passed by reference, it's not necessary to return\n",
    "\n",
    "\n",
    "# calculate records using accumulated histogram, however the four courner is inclusive\n",
    "# hence to accurately calculate the records inside, we need to decrease the lower end by 1\n",
    "def CalculateRecords(dataset_hist_accu, d1_low, d1_up, d2_low, d2_up):\n",
    "    \n",
    "    records_lower_left = 0\n",
    "    if d1_low == 0 or d2_low == 0:\n",
    "        records_lower_left = 0\n",
    "    else:\n",
    "        records_lower_left = dataset_hist_accu[d1_low-1,d2_low-1]\n",
    "    \n",
    "    records_lower_right = 0\n",
    "    if d1_low == 0:\n",
    "        records_lower_right = 0\n",
    "    else:\n",
    "        records_lower_right = dataset_hist_accu[d1_low-1,d2_up]\n",
    "        \n",
    "    records_upper_left = 0\n",
    "    if d2_low == 0:\n",
    "        records_upper_left = 0\n",
    "    else:\n",
    "        records_upper_left = dataset_hist_accu[d1_up,d2_low-1]\n",
    "    \n",
    "    total_records = dataset_hist_accu[d1_up,d2_up] + records_lower_left -  records_lower_right - records_upper_left\n",
    "#     print(\"+ hist[\",d1_up,\",\",d2_up,\"]: \",dataset_hist_accu[d1_up,d2_up])\n",
    "#     print(\"+ hist[\",d1_low,\",\",d2_low,\"]: \",records_lower_left)\n",
    "#     print(\"- hist[\",d1_low,\",\",d2_up,\"]: \",records_lower_right)\n",
    "#     print(\"- hist[\",d1_up,\",\",d2_low,\"]: \",records_upper_left)\n",
    "    return total_records\n",
    "\n",
    "# calculate number of queries using accumulated euler histogram\n",
    "def CalculateQueries(queryset_hist_accu, d1_low, d1_up, d2_low, d2_up):\n",
    "    \n",
    "    # C1\n",
    "    C1 = queryset_hist_accu[d1_low][d2_low][0]\n",
    "    \n",
    "    # sum of C2\n",
    "    SC2 = queryset_hist_accu[d1_low][d2_up][1] - queryset_hist_accu[d1_low][d2_low][1]\n",
    "    \n",
    "    # sum of C3\n",
    "    SC3 = queryset_hist_accu[d1_up][d2_low][2] - queryset_hist_accu[d1_low][d2_low][2]\n",
    "    \n",
    "    # sum of C4\n",
    "    #SC4 = queryset_hist_accu[d1_up][d2_up][3] + queryset_hist_accu[d1_low+1][d2_low+1][3] - queryset_hist_accu[d1_up][d2_low+1][3] - queryset_hist_accu[d1_low+1][d2_up][3]\n",
    "    SC4 = queryset_hist_accu[d1_up][d2_up][3] + queryset_hist_accu[d1_low][d2_low][3] - queryset_hist_accu[d1_up][d2_low][3] - queryset_hist_accu[d1_low][d2_up][3]\n",
    "    \n",
    "#     print(\"d1_low: \", d1_low, \" d1_up: \", d1_up, \" d2_low: \", d2_low, \" d2_up: \", d2_up)\n",
    "#     print(\"C1: +accu hist[\",d1_low,'][',d2_low,'][0]: ', queryset_hist_accu[d1_low][d2_low][0])\n",
    "    \n",
    "#     print(\"SC2: +accu hist[\",d1_low,'][',d2_up,'][1]: ', queryset_hist_accu[d1_low][d2_up][1], \" -accu hist[\",d1_low,'][',d2_low,'][1]: ',queryset_hist_accu[d1_low][d2_low][1])\n",
    "#     print(\"SC3: +accu hist[\",d1_up,'][',d2_low,'][2]: ', queryset_hist_accu[d1_up][d2_low][2], \" -accu hist[\",d1_low,'][',d2_low,'][2]: ',queryset_hist_accu[d1_low][d2_low][2])\n",
    "    \n",
    "#     print(\"SC4: +accu hist[\",d1_up,'][',d2_up,'][3]: ', queryset_hist_accu[d1_up][d2_up][3], \" +accu hist[\",d1_low,'][',d2_low,'][3]: ',queryset_hist_accu[d1_low][d2_low][3], \n",
    "#           \" -accu hist[\",d1_up,'][',d2_low,'][3]: ',queryset_hist_accu[d1_up][d2_low][3], \" -accu hist[\",d1_low,'][',d2_up,'][1]: ', queryset_hist_accu[d1_low][d2_up][3])\n",
    "    \n",
    "#     print(\"C1: \",C1, \" SC2: \", SC2, \" SC3: \", SC3, \" SC4: \", SC4)\n",
    "    total_queries = C1 + SC2 + SC3 + SC4\n",
    "    \n",
    "    return total_queries\n",
    "    \n",
    "\n",
    "# search for the split position that balance the cost most\n",
    "# search for the first dimension first, then the second, use the one that has the minimum difference\n",
    "# @partition: a kdnode[0/1][k][0/1]: boundary/count  dimension min/max\n",
    "def QueryPrefixSumHistForMedian(partition, dataset_hist_accu, queryset_hist_accu, m, n, domain):\n",
    "    \n",
    "    dim1_step = (domain[0][1]-domain[0][0])/m\n",
    "    dim2_step = (domain[1][1]-domain[1][0])/n\n",
    "    \n",
    "    dim1_index_min = int((partition[0][0][0] - domain[0][0]) / dim1_step)\n",
    "    dim1_index_max = int((partition[0][0][1] - domain[0][0]) / dim1_step)\n",
    "    dim2_index_min = int((partition[0][1][0] - domain[1][0]) / dim1_step) # inclusive\n",
    "    dim2_index_max = int((partition[0][1][1] - domain[1][0]) / dim1_step) # inclusive\n",
    "    \n",
    "    if dim1_index_min >= m:\n",
    "        dim1_index_min = m-1\n",
    "    if dim1_index_max >= m:\n",
    "        dim1_index_max = m-1\n",
    "    if dim2_index_min >= n:\n",
    "        dim2_index_min = n-1\n",
    "    if dim2_index_max >= n:\n",
    "        dim2_index_max = n-1\n",
    "    \n",
    "    # find the original cost (queries * records)\n",
    "    queries = CalculateQueries(queryset_hist_accu, dim1_index_min, dim1_index_max, dim2_index_min, dim2_index_max)\n",
    "    original_cost =  queries * partition[1]\n",
    "    \n",
    "    # the cost difference for the resulting sub-partitions\n",
    "    min_difference_dim1 = 0\n",
    "    min_difference_dim2 = 0\n",
    "    \n",
    "    split_pos_dim1 = 0\n",
    "    split_pos_dim2 = 0\n",
    "    \n",
    "    # ========================================\n",
    "    \n",
    "    # binary search for the first dimension\n",
    "#     print(\"=== first dimension ===\")\n",
    "    \n",
    "    lower_index = dim1_index_min\n",
    "    upper_index = dim1_index_max\n",
    "    \n",
    "    lower_records_dim1 = 0\n",
    "    upper_records_dim1 = 0\n",
    "    \n",
    "    reduced_cost_dim1 = 0\n",
    "    \n",
    "    first_split = True\n",
    "    last_cost_diff_abs = 0\n",
    "    \n",
    "    while lower_index < upper_index:\n",
    "#         print(\"--- loop ---\")\n",
    "        mid_index = int((lower_index + upper_index)/2)\n",
    "        if mid_index == lower_index or mid_index == upper_index:\n",
    "            break\n",
    "        \n",
    "        # calculate cost; include lower but not higher\n",
    "        records_lower = CalculateRecords(dataset_hist_accu, dim1_index_min, mid_index, dim2_index_min, dim2_index_max)\n",
    "        records_upper = CalculateRecords(dataset_hist_accu, mid_index + 1, dim1_index_max, dim2_index_min, dim2_index_max) \n",
    "#         print('records low: ', records_lower)\n",
    "#         print('records up: ', records_upper)\n",
    "        \n",
    "        queries_lower = CalculateQueries(queryset_hist_accu, dim1_index_min,  mid_index, dim2_index_min, dim2_index_max)\n",
    "        queries_upper = CalculateQueries(queryset_hist_accu, mid_index+1,  dim1_index_max, dim2_index_min, dim2_index_max)\n",
    "#         print('queries low: ', queries_lower)\n",
    "#         print('queries up: ', queries_upper)\n",
    "        \n",
    "        cost_lower = queries_lower * records_lower\n",
    "        cost_upper = queries_upper * records_upper\n",
    "#         print('cost low: ', cost_lower)\n",
    "#         print('cost up: ', cost_upper)\n",
    "        \n",
    "        cost_diff = cost_lower - cost_upper\n",
    "        cost_diff_abs = abs(cost_diff)\n",
    "#         print('diff: ', cost_diff_abs)\n",
    "        \n",
    "        if first_split:\n",
    "            last_cost_diff_abs = cost_diff_abs\n",
    "            split_pos_dim1 = mid_index\n",
    "            lower_records_dim1 = records_lower\n",
    "            upper_records_dim1 = records_upper\n",
    "            reduced_cost_dim1 = cost_lower + cost_upper\n",
    "            first_split = False\n",
    "        \n",
    "        # decide if continue the loop\n",
    "        #if cost_diff_abs == 0 or cost_diff_abs > last_cost_diff_abs:\n",
    "        if cost_diff_abs == 0:\n",
    "            # stop the loop\n",
    "            break\n",
    "        else: # continue the loop\n",
    "            last_cost_diff_abs = cost_diff_abs\n",
    "            split_pos_dim1 = mid_index\n",
    "            lower_records_dim1 = records_lower\n",
    "            upper_records_dim1 = records_upper\n",
    "            reduced_cost_dim1 = cost_lower + cost_upper\n",
    "            \n",
    "            if cost_diff < 0: # cost_lower is smaller, the split should move upward\n",
    "                lower_index = mid_index + 1\n",
    "                \n",
    "            if cost_diff > 0: # cost_lower is larger, the split should move downward\n",
    "                upper_index = mid_index\n",
    "                \n",
    "#         print(\"current mid index: \", mid_index)        \n",
    "        \n",
    "    min_difference_dim1 = last_cost_diff_abs\n",
    "#     print(\"dim: \", 0, \" diff: \", min_difference_dim1, \" pos: \", split_pos_dim1)\n",
    "    \n",
    "    \n",
    "    # binary search for the second dimension\n",
    "#     print(\"=== second dimension ===\")\n",
    "    \n",
    "    lower_index = dim2_index_min\n",
    "    upper_index = dim2_index_max\n",
    "    \n",
    "    lower_records_dim2 = 0\n",
    "    upper_records_dim2 = 0\n",
    "    \n",
    "    reduced_cost_dim2 = 0\n",
    "    \n",
    "    first_split = True\n",
    "    last_cost_diff_abs = 0\n",
    "    \n",
    "    while lower_index < upper_index:\n",
    "        \n",
    "#         print(\"--- loop ---\")\n",
    "        \n",
    "        mid_index = int((lower_index + upper_index)/2)\n",
    "        if mid_index == lower_index or mid_index == upper_index:\n",
    "            break\n",
    "        \n",
    "        # calculate cost  !!! change here!!!!!!!!!\n",
    "        records_lower = CalculateRecords(dataset_hist_accu, dim1_index_min, dim1_index_max, dim2_index_min, mid_index)\n",
    "        records_upper = CalculateRecords(dataset_hist_accu, dim1_index_min, dim1_index_max, mid_index+1, dim2_index_max)      \n",
    "#         print('records low: ', records_lower)\n",
    "#         print('records up: ', records_upper)\n",
    "        \n",
    "        queries_lower = CalculateQueries(queryset_hist_accu, dim1_index_min, dim1_index_max, dim2_index_min,  mid_index)\n",
    "        queries_upper = CalculateQueries(queryset_hist_accu, dim1_index_min, dim1_index_max, mid_index+1,  dim2_index_max)\n",
    "#         print('queries low: ', queries_lower)\n",
    "#         print('queries up: ', queries_upper)\n",
    "        \n",
    "        cost_lower = queries_lower * records_lower\n",
    "        cost_upper = queries_upper * records_upper\n",
    "#         print('cost low: ', cost_lower)\n",
    "#         print('cost up: ', cost_upper)\n",
    "        \n",
    "        cost_diff = cost_lower - cost_upper\n",
    "        cost_diff_abs = abs(cost_diff)\n",
    "#         print('diff: ', cost_diff_abs)\n",
    "        \n",
    "        if first_split:\n",
    "            last_cost_diff_abs = cost_diff_abs\n",
    "            split_pos_dim2 = mid_index\n",
    "            lower_records_dim2 = records_lower\n",
    "            upper_records_dim2 = records_upper\n",
    "            reduced_cost_dim2 = cost_lower + cost_upper\n",
    "            \n",
    "            first_split = False\n",
    "        \n",
    "        # decide if continue the loop\n",
    "        #if cost_diff_abs == 0 or cost_diff_abs > last_cost_diff_abs:\n",
    "        if cost_diff_abs == 0:\n",
    "            # stop the loop\n",
    "            break\n",
    "        else: # continue the loop\n",
    "            last_cost_diff_abs = cost_diff_abs\n",
    "            split_pos_dim2 = mid_index\n",
    "            lower_records_dim2 = records_lower\n",
    "            upper_records_dim2 = records_upper\n",
    "            reduced_cost_dim2 = cost_lower + cost_upper\n",
    "            \n",
    "            if cost_diff < 0: # cost_lower is smaller, the split should move upward\n",
    "                lower_index = mid_index + 1\n",
    "                \n",
    "            if cost_diff > 0: # cost_lower is larger, the split should move downward\n",
    "                upper_index = mid_index\n",
    "                \n",
    "#         print(\"current mid index: \", mid_index)\n",
    "        \n",
    "    # ============================================    \n",
    "    min_difference_dim2 = last_cost_diff_abs\n",
    "#     print(\"dim: \", 1, \" diff: \", min_difference_dim2, \" pos: \", split_pos_dim2)\n",
    "    \n",
    "    # choose the smaller difference one to return\n",
    "    if min_difference_dim1 < min_difference_dim2:\n",
    "        split_pos_dim1 = (split_pos_dim1 + 1) * dim1_step + domain[0][0] # from index to value\n",
    "        return 0, min_difference_dim1, split_pos_dim1, lower_records_dim1, upper_records_dim1, original_cost, reduced_cost_dim1\n",
    "    else:\n",
    "        split_pos_dim2 = (split_pos_dim2 + 1) * dim2_step + domain[1][0] # from index to value\n",
    "        return 1, min_difference_dim2, split_pos_dim2, lower_records_dim2, upper_records_dim2, original_cost, reduced_cost_dim2\n",
    "    \n",
    "# 2D\n",
    "def FindExactRecordsAmount(dataset, domain):\n",
    "    count = 0\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i,0] >= domain[0][0] and dataset[i,0] < domain[0][1] and dataset[i,1] >= domain[1][0] and dataset[i,1] < domain[1][1]:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # = = = Unit Test = = =\n",
    "# # test for insert\n",
    "# queryset_hist = GetEmptyEularHist(10, 10)\n",
    "# queryset_hist_accu = GetEmptyEularHist(10, 10)\n",
    "\n",
    "# tiny_domain = [[0,10],[0,10]]\n",
    "\n",
    "# tiny_queryset=[\n",
    "#     [[2,6],[2,6]],\n",
    "#     [[4,8],[4,7]]\n",
    "# ]\n",
    "\n",
    "# print(\" before insertion \")\n",
    "# for i in range(len(queryset_hist)):\n",
    "#     print(queryset_hist[i])\n",
    "# print(\"= = = = = = = = = = = = =\")\n",
    "# for i in range(len(queryset_hist_accu)):\n",
    "#     print(queryset_hist_accu[i])\n",
    "# print(\" after insertion 1\")\n",
    "# InserQueryIntoEulerHist(tiny_queryset[0], queryset_hist, queryset_hist_accu, tiny_domain, 10, 10)\n",
    "# for i in range(len(queryset_hist)):\n",
    "#     print(queryset_hist[i])\n",
    "# print(\"= = = = = = = = = = = = =\")\n",
    "# for i in range(len(queryset_hist_accu)):\n",
    "#     print(queryset_hist_accu[i])\n",
    "# print(\" after insertion 2\")\n",
    "# InserQueryIntoEulerHist(tiny_queryset[1], queryset_hist, queryset_hist_accu, tiny_domain, 10, 10)\n",
    "# for i in range(len(queryset_hist)):\n",
    "#     print(queryset_hist[i])\n",
    "# print(\"= = = = = = = = = = = = =\")\n",
    "# for i in range(len(queryset_hist_accu)):\n",
    "#     print(queryset_hist_accu[i])\n",
    "# print(\"= = = = = = = = = = = = =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # = = = Unit Test = = =\n",
    "# # Test the accuracy using tiny dataset 10*10 and tiny queryset\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\")) # make the width full\n",
    "\n",
    "# tiny_dataset = []\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         tiny_dataset.append([i,j])\n",
    "# tiny_dataset = np.array(tiny_dataset)\n",
    "# # print(tiny_dataset)\n",
    "\n",
    "# tiny_domain = [[0,10],[0,10]]\n",
    "\n",
    "# hist = CreateHist(tiny_dataset, 10, 10, tiny_domain)\n",
    "# hist = np.array(hist)\n",
    "# print(hist)\n",
    "\n",
    "# accuhist =  CreatePrefixSumHist(hist)\n",
    "# print(accuhist)\n",
    "\n",
    "# tiny_queryset=[\n",
    "#     [[2,6],[2,6]],\n",
    "#     [[4,8],[4,7]]\n",
    "# ]\n",
    "\n",
    "# eularhist = CreateEulerHist(tiny_queryset, 10, 10, tiny_domain)\n",
    "# eularhist = np.array(eularhist)\n",
    "# print(eularhist)\n",
    "# accueularhist = CreateAccumulateEulerHist(eularhist)\n",
    "# print(accueularhist)\n",
    "\n",
    "# euler_hist_list = np.copy(eularhist)\n",
    "# euler_hist_list = euler_hist_list.tolist()\n",
    "# for i in range(len(euler_hist_list)):\n",
    "#     print(euler_hist_list[i])\n",
    "\n",
    "# print(\"= = = = = = = = = = = = =\")\n",
    "# accu_euler_hist_list = np.copy(accueularhist)\n",
    "# accu_euler_hist_list = accu_euler_hist_list.tolist()\n",
    "# for i in range(len(accu_euler_hist_list)):\n",
    "#     print(accu_euler_hist_list[i])\n",
    "\n",
    "# tiny_partition = [\n",
    "#     [[0,7],[0,9]],\n",
    "#     [70]\n",
    "# ]\n",
    "\n",
    "# dim, diff, pos, lower_records, upper_records, original_cost, reduced_cost = QueryPrefixSumHistForMedian(tiny_partition, accuhist, accueularhist, 10, 10, tiny_domain)\n",
    "# print(\"Final:  dim: \", dim, \" diff: \", diff, \" pos: \", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the AQWA approach （only in 2D）\n",
    "\n",
    "# step 1: Initialized as a KD-Tree (stop when any resulting partitions will less than M)\n",
    "# step 2: add more data (such that a partition may be able to split)\n",
    "# step 3: for every arrived query: calculated whether to split. ( When and Where to split)\n",
    "\n",
    "# @kdnodes: the overfull kdnodes, initialized with old dataset using KDT, then insert new data\n",
    "\n",
    "import copy\n",
    "def AQWAPartition(kdnodes, dataset, domains, threshold, queries, m=100, n=100):\n",
    "    \n",
    "    dims = dataset.shape[1]\n",
    "    \n",
    "#     KDT_kdnodes = TraditionalKDTree(old_dataset, 0, domains, threshold, 0)\n",
    "#     print(\"finish basic KDT partitioning\")\n",
    "    \n",
    "#     kdnodes = AppendData2Partitions(new_dataset,KDT_kdnodes)\n",
    "#     print(\"finish appending data to existing partitions\")\n",
    "    \n",
    "    # create histogram for dataset (old + new)\n",
    "    dataset_hist = CreateHist(dataset, m, n, domains)\n",
    "    dataset_hist_accu = CreatePrefixSumHist(dataset_hist)\n",
    "    \n",
    "    # create an empty eular histogram for query\n",
    "    queryset_hist = GetEmptyEularHist(m, n)\n",
    "    queryset_hist_accu = GetEmptyEularHist(m, n)\n",
    "    \n",
    "    # priority queue, used for partitions' cost benefitr\n",
    "    partition_cost_reduction = {} # the priority queue in python is really not good, so I use a dictionary\n",
    "    \n",
    "    # a dictionary to maintain the candidate split info\n",
    "    candidate_split_partition_info = {}\n",
    "    \n",
    "    # using queries to split the existing partitions\n",
    "    for i in range(len(queries)):\n",
    "        \n",
    "        print(\"Processing queries No. \", i)\n",
    "        # update the query's eular histogram\n",
    "        #queryset_hist,queryset_hist_accu = InserQueryIntoEulerHist(queries[i], queryset_hist, queryset_hist_accu, domains, m, n) # the old version\n",
    "        InserQueryIntoEulerHist(queries[i], queryset_hist, queryset_hist_accu, domains, m, n) # the optimized version, do not need to return\n",
    "        \n",
    "        # 1. find out the overlap partitions\n",
    "        current_cost = 0\n",
    "        # check for intersection for each kdnode\n",
    "        for j in range(len(kdnodes)):\n",
    "            intersection_tag = True\n",
    "            # for each dimension\n",
    "            for k in range(dims):\n",
    "                # an intersection holds if it intersecs in all dimensions\n",
    "                #print(\"k: \", k)\n",
    "                #print(\"j: \", j)\n",
    "                #print('queries[i][k][0] = queries[',i,'][',k,'][0]= ',queries[i][k][0])\n",
    "                #print('queries[i][k][1] = queries[',i,'][',k,'][1]= ',queries[i][k][1])\n",
    "                #print('kdnodes[j][0][k][1] = kdnodes[',j,'][0][',k,'][1]= ',kdnodes[j][0][k][1])\n",
    "                #print('kdnodes[j][0][k][1] = kdnodes[',j,'][0][',k,'][0]= ',kdnodes[j][0][k][0])        \n",
    "                if queries[i][k][0] > kdnodes[j][0][k][1] or queries[i][k][1] < kdnodes[j][0][k][0]:\n",
    "                    intersection_tag = False\n",
    "                    break\n",
    "            # if the query intersect with this kdnode\n",
    "            if intersection_tag:\n",
    "                current_cost += kdnodes[j][1] # number of records in this partition\n",
    "                \n",
    "            # 2. calculate the best split position of each overlap partition\n",
    "            split_dim, diff, split_pos, lower_records, upper_records, original_cost, reduced_cost = QueryPrefixSumHistForMedian(kdnodes[j], dataset_hist_accu, queryset_hist_accu, m, n, domains)\n",
    "        \n",
    "            # 3. check if the resulted split will make sub-partition less than M, if yes, do not insert into the priority queue\n",
    "            if lower_records < threshold or upper_records < threshold:\n",
    "                pass\n",
    "            else:\n",
    "            # 4. calculate the cost deduction after split and insert (cost deduction - IOcost) into the priority queue\n",
    "                benefit = original_cost - reduced_cost - 2 * kdnodes[j][1]\n",
    "            # (if the partition is already inside the queue, update its value)\n",
    "                if benefit > 0:\n",
    "                    partition_cost_reduction.update({j : benefit})\n",
    "                    candidate_split_partition_info.update({j : [split_dim, diff, split_pos, lower_records, upper_records, original_cost, reduced_cost]})\n",
    "       \n",
    "        # after the query\n",
    "        # 5. split the top partition? after split, keep the split partition half and add 1 more at the end of the kdnodes, so the index unchanged\n",
    "        \n",
    "        # if noting to split, just continue the next loop\n",
    "        if len(partition_cost_reduction) == 0:\n",
    "            continue\n",
    "            \n",
    "        max_benefit_partition_index = max(partition_cost_reduction, key=partition_cost_reduction.get)\n",
    "        \n",
    "        print(\"(before) perform split at query: \", i, \" for kdnodes: \", max_benefit_partition_index, \" node info: \", kdnodes[max_benefit_partition_index])\n",
    "        \n",
    "        split_dim, diff, split_pos, lower_records, upper_records, original_cost, reduced_cost = candidate_split_partition_info[max_benefit_partition_index]\n",
    "        \n",
    "        # so each query will only split 1 partition at most?  --> I think Yes\n",
    "        # do split the greatest\n",
    "        #new_sub_partition = kdnodes[max_benefit_partition_index].copy() # this copy is not deep!!!\n",
    "        new_sub_partition = copy.deepcopy(kdnodes[max_benefit_partition_index]) # using this one instead!!!\n",
    "        \n",
    "        # take this as lower\n",
    "        kdnodes[max_benefit_partition_index][0][split_dim][1] = split_pos\n",
    "        # take this as upper\n",
    "        new_sub_partition[0][split_dim][0] = split_pos\n",
    "        \n",
    "        # find the new exact records for the resulting 2 partitions\n",
    "        old_partition_count = FindExactRecordsAmount(dataset, kdnodes[max_benefit_partition_index][0])\n",
    "        new_partition_count = kdnodes[max_benefit_partition_index][1] - old_partition_count\n",
    "        \n",
    "        kdnodes[max_benefit_partition_index][1] = old_partition_count\n",
    "        new_sub_partition[1] = new_partition_count\n",
    "        \n",
    "        # insert 1 additional partition into de kdnodes, and remove the partition from priority queue\n",
    "        kdnodes.append(new_sub_partition)\n",
    "        print(\"(after) perform split at query: \", i, \" for kdnodes: \", max_benefit_partition_index,\" node info: \", kdnodes[max_benefit_partition_index])\n",
    "        print(\"split dimension: \", split_dim, \" split position: \", split_pos)\n",
    "        print(\"new partition: \", new_sub_partition)\n",
    "        \n",
    "        # after split, remove the candidate split info\n",
    "        del partition_cost_reduction[max_benefit_partition_index] # use del keyword to delete an entry in dictionary !!!\n",
    "        del candidate_split_partition_info[max_benefit_partition_index]\n",
    "\n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kdnodes[i][0/1][k][0/1]  the nodes, domain/count dimension min/max\n",
    "def ExportKDNodes(kdnodes, path='C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Legacy/data/AQWA_partitions_overfull.csv'):\n",
    "    nodes = []\n",
    "    for i in range(len(kdnodes)):\n",
    "        row = []\n",
    "        for k in range(len(kdnodes[i][0])):\n",
    "            row.append(kdnodes[i][0][k][0])\n",
    "            row.append(kdnodes[i][0][k][1])\n",
    "        row.append(kdnodes[i][1])\n",
    "        nodes.append(row)\n",
    "    nodes = np.array(nodes)\n",
    "    np.savetxt(path, nodes, delimiter=',')\n",
    "    return nodes\n",
    "\n",
    "def ImportKDNodes(path='C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Legacy/data/AQWA_partitions_overfull.csv'):\n",
    "    nodes = genfromtxt(path, delimiter=',')\n",
    "    kdnodes = []\n",
    "    dims = int(nodes.shape[1]/2)\n",
    "    for i in range(nodes.shape[0]):\n",
    "        row = []\n",
    "        for k in range(dims):\n",
    "            row.append([nodes[i,2*k], nodes[i,2*k+1]])\n",
    "        kdnodes.append([row,nodes[i,-1]])\n",
    "    \n",
    "    while True:\n",
    "        found_tag = False\n",
    "        for i in range(len(kdnodes)):\n",
    "            if isinstance(kdnodes[i][0], list) == False:\n",
    "                print(kdnodes[i], \" at index: \", i)\n",
    "                kdnodes.pop(i)\n",
    "                found_tag = True\n",
    "                break\n",
    "        if found_tag == False:\n",
    "            break\n",
    "            \n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exported_kdnodes = ExportKDNodes(overfull_kdnodes)\n",
    "# imported_kdnodes = ImportKDNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KDPartition(dataset, current_dim, data_threshold, root_node, kdnode_dict, accu_count_list):\n",
    "    \n",
    "    current_size = len(dataset)\n",
    "    if current_size <= data_threshold:\n",
    "        return [root_node] # here we assume the children nodes are -1 and -1\n",
    "        \n",
    "    # try partition this node into 2\n",
    "    median = np.median(dataset[:,current_dim])\n",
    "    \n",
    "    sub_domains1 = np.copy(root_node[0])\n",
    "    sub_domains1[current_dim][1] = median\n",
    "    sub_domains2 = np.copy(root_node[0])\n",
    "    sub_domains2[current_dim][0] = median\n",
    "    \n",
    "    sub_dataset1 = dataset[dataset[:,current_dim] <= median]\n",
    "    sub_dataset2 = dataset[dataset[:,current_dim] > median]\n",
    "    \n",
    "    if len(sub_dataset1) < data_threshold or len(sub_dataset2) < data_threshold:\n",
    "        return [root_node]\n",
    "    \n",
    "    sub_kdnode_1 = [sub_domains1, len(sub_dataset1), accu_count_list[0] + 1, root_node[-4], -1, -1]\n",
    "    sub_kdnode_2 = [sub_domains2, len(sub_dataset2), accu_count_list[0] + 2, root_node[-4], -1, -1]\n",
    "    \n",
    "    root_node[-2] = sub_kdnode_1[-4]\n",
    "    root_node[-1] = sub_kdnode_2[-4]\n",
    "    \n",
    "    kdnode_dict.update({sub_kdnode_1[-4]: sub_kdnode_1})\n",
    "    kdnode_dict.update({sub_kdnode_2[-4]: sub_kdnode_2})\n",
    "    \n",
    "    accu_count_list[0] += 2\n",
    "    \n",
    "    current_dim += 1\n",
    "    if current_dim >= len(root_node[0]):\n",
    "        current_dim %= len(root_node[0])\n",
    "        \n",
    "    kdnodes = []\n",
    "    kdnodes += KDPartition(sub_dataset1, current_dim, data_threshold, sub_kdnode_1, kdnode_dict, accu_count_list)\n",
    "    kdnodes += KDPartition(sub_dataset2, current_dim, data_threshold, sub_kdnode_2, kdnode_dict, accu_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split by random\n",
    "def dataset_split(dataset, initial_percentage = 0.5, path1, path2):\n",
    "    np.random.shuffle(old_dataset)\n",
    "    train_length = int(initial_percentage*len(dataset))\n",
    "    initial_dataset = dataset[0:train_length]\n",
    "    subsequent_dataset = dataset[train_length:]\n",
    "    \n",
    "    # consider to save the 2 dataset\n",
    "    np.savetxt('C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Robust/dataset/AQWA_init.csv', initial_dataset)\n",
    "    np.savetxt('C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Robust/dataset/AQWA_subseq.csv', subsequent_dataset)\n",
    "    \n",
    "    return initial_dataset, subsequent_dataset\n",
    "\n",
    "# using KDT partition\n",
    "def AQWA_Initialization(initial_dataset, subsequent_dataset, domain, data_threshold):\n",
    "     \n",
    "    # create KDT partitions using KDT and the initial dataset\n",
    "    root_node, kdnode_dict, accu_count_list = [domain, len(initial_dataset), 0, -1, -1, -1], {}, [0]\n",
    "    KDT_kdnodes = KDPartition(initial_dataset, 0, data_threshold, root_node, kdnode_dict, accu_count_list):\n",
    "    \n",
    "    # insert subsequent_dataset (append each record from subsequent_datase to its partition), this could take a while\n",
    "    overfull_kdnodes = AppendData2Partitions(subsequent_dataset, KDT_kdnodes) # training time 2134.4680318832397\n",
    "    \n",
    "    return overfull_kdnodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
