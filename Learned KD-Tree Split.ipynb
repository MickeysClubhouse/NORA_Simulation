{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "# import the synthectic query workload\n",
    "# long range: from -180 to 180, the first column\n",
    "# lat range: from -90 to 90, the second column\n",
    "query_bound_dim1 = genfromtxt('/Users/lizhe/Desktop/LearnedKDTree/DataAndWorkload/SyntheticWorkload/Dim1_QueryBound_Tweet_C10_P10_S5.csv', delimiter=',')\n",
    "query_bound_dim2 = genfromtxt('/Users/lizhe/Desktop/LearnedKDTree/DataAndWorkload/SyntheticWorkload/Dim2_QueryBound_Tweet_C10_P10_S5.csv', delimiter=',')\n",
    "# the twitter dataset\n",
    "dataset = genfromtxt(\"/Users/lizhe/Library/Mobile Documents/com~apple~CloudDocs/SortedSingleDimPOIs2.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method to generate kdnodes, where each node should not exceed the page size threshold\n",
    "# @dataset, the origin dataset, contains each dims value in each row\n",
    "# @query_bounds, the [] that contains each dim in each row\n",
    "# @threshold, the page size threshold\n",
    "def LearnedKDTree(dataset, query_bound, Dimorder, domains, threshold=32000):\n",
    "    \n",
    "    # load query rectangles\n",
    "    # scan to balance the cross of rectangles and data\n",
    "    # save split position\n",
    "    \n",
    "    currentDim = 0\n",
    "    \n",
    "    kdnodes = ResuriveDivide(dataset, query_bound, currentDim, Dimorder, domains, threshold, 0)\n",
    "    \n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asssumption: the query boundings will not overlap\n",
    "\n",
    "# divide the KD-Tree recursively\n",
    "# @dataset, should contains the data only in this subnode; numpy object\n",
    "# @query_bound, should contains all the bounds; numpy object\n",
    "# @currentDim, the dimension this iteration should focus on, an index in the Dimorder; integer\n",
    "# @domains, the current domain of the nodes of every dimension [first lower, second upper],[]...; array object\n",
    "def ResuriveDivide(dataset, query_bound, currentDim, Dimorder, domains, threshold, level):\n",
    "    \n",
    "    print(\"level: \",level)\n",
    "    print(\"dataset size: \",len(dataset))\n",
    "    \n",
    "    # check if the threshold is already satisfied\n",
    "    total_size = len(dataset)\n",
    "    if total_size <= threshold:\n",
    "        # the kdnodes should be an global object outside the function\n",
    "        kdnodes = []\n",
    "        kdnodes.append([domains,total_size])\n",
    "        return kdnodes\n",
    "    \n",
    "    # the current dimension\n",
    "    divideDim = Dimorder[currentDim]\n",
    "    \n",
    "    # sort according to the current dimension\n",
    "    dataset = dataset[dataset[:,divideDim].argsort()]\n",
    "    \n",
    "    # find the medium\n",
    "    medium = dataset[int(total_size/2),divideDim]\n",
    "    medium_low = domains[divideDim][0]\n",
    "    medium_up = domains[divideDim][1]\n",
    "    \n",
    "    # start check split position from the medium\n",
    "    split_position = int(total_size/2)\n",
    "    split_low = 0\n",
    "    split_up = total_size\n",
    "    \n",
    "    # check if the split position intersect some query boundings in this dim\n",
    "    for i in range(len(query_bound[divideDim])):\n",
    "        \n",
    "        # if intersect some query bounds\n",
    "        if medium > query_bound[divideDim][i][0] and medium < query_bound[divideDim][i][1]:\n",
    "            \n",
    "            # check if the two end already exceeds domain\n",
    "            if query_bound[divideDim][i][0] < domains[divideDim][0] and query_bound[divideDim][i][1] > domains[divideDim][1]:\n",
    "                break;\n",
    "            \n",
    "            else:\n",
    "                if query_bound[divideDim][i][0] > domains[divideDim][0]:\n",
    "                # get the number of records from medium to the end\n",
    "                    for j in range(split_position-1,-1,-1):\n",
    "                        if dataset[j][divideDim] <= query_bound[divideDim][i][0]:\n",
    "                            split_low = j\n",
    "                            medium_low = dataset[split_low,divideDim]\n",
    "                            break\n",
    "                \n",
    "                if query_bound[divideDim][i][1] < domains[divideDim][1]:\n",
    "                # get the number of records from medium to the end\n",
    "                    for j in range(split_position,total_size,1):\n",
    "                        if dataset[j][divideDim] >= query_bound[divideDim][i][1]:\n",
    "                            split_up = j\n",
    "                            medium_up = dataset[split_up,divideDim]\n",
    "                            break\n",
    "                \n",
    "            # if not exceeds then choose the one that is closest from the medium (in terms of #records!)\n",
    "            if (total_size/2) - split_low < (split_up - total_size/2) and split_low != 0:\n",
    "                split_position = split_low\n",
    "                medium = medium_low\n",
    "            elif (total_size/2) - split_low >= (split_up - total_size/2) and split_up != total_size :\n",
    "                split_position = split_up\n",
    "                medium = medium_up\n",
    "            \n",
    "            # after handle the overlap bounding, we can skip the remaining, as there will be at most 1 as assumned\n",
    "            break;\n",
    "            \n",
    "    # split the dataset according to the split position\n",
    "    sub_dataset1 = dataset[0:split_position,:]\n",
    "    sub_dataset2 = dataset[split_position:-1,:]\n",
    "    \n",
    "    # change the domains\n",
    "    sub_domains1 = np.copy(domains)\n",
    "    sub_domains1[divideDim][1] = medium\n",
    "    sub_domains2 = np.copy(domains)\n",
    "    sub_domains2[divideDim][0] = medium\n",
    "    \n",
    "    # change the divideDim\n",
    "    currentDim += 1\n",
    "    if currentDim >= len(Dimorder):\n",
    "        currentDim %= len(Dimorder)\n",
    "    \n",
    "    # used to see the current depth\n",
    "    level += 1\n",
    "    \n",
    "    # recursion\n",
    "    kdnodes = []\n",
    "    kdnodes.extend(ResuriveDivide(sub_dataset1, query_bound, currentDim, Dimorder, sub_domains1, threshold, level))\n",
    "    kdnodes.extend(ResuriveDivide(sub_dataset2, query_bound, currentDim, Dimorder, sub_domains2, threshold, level))\n",
    "    \n",
    "    print(\"kdnodes: \",len(kdnodes))\n",
    "    \n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:  0\n",
      "dataset size:  1157570\n",
      "level:  1\n",
      "dataset size:  578785\n",
      "level:  2\n",
      "dataset size:  309798\n",
      "level:  3\n",
      "dataset size:  154899\n",
      "level:  4\n",
      "dataset size:  4775\n",
      "level:  4\n",
      "dataset size:  150123\n",
      "level:  5\n",
      "dataset size:  75061\n",
      "level:  6\n",
      "dataset size:  37530\n",
      "level:  7\n",
      "dataset size:  31844\n",
      "level:  7\n",
      "dataset size:  5685\n",
      "kdnodes:  2\n",
      "level:  6\n",
      "dataset size:  37530\n",
      "level:  7\n",
      "dataset size:  18765\n",
      "level:  7\n",
      "dataset size:  18764\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  5\n",
      "dataset size:  75061\n",
      "level:  6\n",
      "dataset size:  37530\n",
      "level:  7\n",
      "dataset size:  18765\n",
      "level:  7\n",
      "dataset size:  18764\n",
      "kdnodes:  2\n",
      "level:  6\n",
      "dataset size:  37530\n",
      "level:  7\n",
      "dataset size:  18765\n",
      "level:  7\n",
      "dataset size:  18764\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "kdnodes:  8\n",
      "kdnodes:  9\n",
      "level:  3\n",
      "dataset size:  154898\n",
      "level:  4\n",
      "dataset size:  77449\n",
      "level:  5\n",
      "dataset size:  75595\n",
      "level:  6\n",
      "dataset size:  37797\n",
      "level:  7\n",
      "dataset size:  18898\n",
      "level:  7\n",
      "dataset size:  18898\n",
      "kdnodes:  2\n",
      "level:  6\n",
      "dataset size:  37797\n",
      "level:  7\n",
      "dataset size:  9\n",
      "level:  7\n",
      "dataset size:  37787\n",
      "level:  8\n",
      "dataset size:  18893\n",
      "level:  8\n",
      "dataset size:  18893\n",
      "kdnodes:  2\n",
      "kdnodes:  3\n",
      "kdnodes:  5\n",
      "level:  5\n",
      "dataset size:  1853\n",
      "kdnodes:  6\n",
      "level:  4\n",
      "dataset size:  77448\n",
      "level:  5\n",
      "dataset size:  32223\n",
      "level:  6\n",
      "dataset size:  1927\n",
      "level:  6\n",
      "dataset size:  30295\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  45224\n",
      "level:  6\n",
      "dataset size:  22612\n",
      "level:  6\n",
      "dataset size:  22611\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "kdnodes:  10\n",
      "kdnodes:  19\n",
      "level:  2\n",
      "dataset size:  268986\n",
      "level:  3\n",
      "dataset size:  130120\n",
      "level:  4\n",
      "dataset size:  65060\n",
      "level:  5\n",
      "dataset size:  32530\n",
      "level:  6\n",
      "dataset size:  16265\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  32529\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  4\n",
      "dataset size:  65059\n",
      "level:  5\n",
      "dataset size:  32529\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  32529\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "level:  6\n",
      "dataset size:  16264\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "kdnodes:  8\n",
      "level:  3\n",
      "dataset size:  138865\n",
      "level:  4\n",
      "dataset size:  69432\n",
      "level:  5\n",
      "dataset size:  67523\n",
      "level:  6\n",
      "dataset size:  33761\n",
      "level:  7\n",
      "dataset size:  16880\n",
      "level:  7\n",
      "dataset size:  16880\n",
      "kdnodes:  2\n",
      "level:  6\n",
      "dataset size:  33761\n",
      "level:  7\n",
      "dataset size:  16880\n",
      "level:  7\n",
      "dataset size:  16880\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  5\n",
      "dataset size:  1908\n",
      "kdnodes:  5\n",
      "level:  4\n",
      "dataset size:  69432\n",
      "level:  5\n",
      "dataset size:  67110\n",
      "level:  6\n",
      "dataset size:  33555\n",
      "level:  7\n",
      "dataset size:  16777\n",
      "level:  7\n",
      "dataset size:  16777\n",
      "kdnodes:  2\n",
      "level:  6\n",
      "dataset size:  33554\n",
      "level:  7\n",
      "dataset size:  16777\n",
      "level:  7\n",
      "dataset size:  16776\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  5\n",
      "dataset size:  2321\n",
      "kdnodes:  5\n",
      "kdnodes:  10\n",
      "kdnodes:  18\n",
      "kdnodes:  37\n",
      "level:  1\n",
      "dataset size:  578784\n",
      "level:  2\n",
      "dataset size:  254957\n",
      "level:  3\n",
      "dataset size:  127478\n",
      "level:  4\n",
      "dataset size:  63739\n",
      "level:  5\n",
      "dataset size:  31869\n",
      "level:  5\n",
      "dataset size:  31869\n",
      "kdnodes:  2\n",
      "level:  4\n",
      "dataset size:  63738\n",
      "level:  5\n",
      "dataset size:  31869\n",
      "level:  5\n",
      "dataset size:  31868\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  3\n",
      "dataset size:  127478\n",
      "level:  4\n",
      "dataset size:  51747\n",
      "level:  5\n",
      "dataset size:  22693\n",
      "level:  5\n",
      "dataset size:  29053\n",
      "kdnodes:  2\n",
      "level:  4\n",
      "dataset size:  75730\n",
      "level:  5\n",
      "dataset size:  27282\n",
      "level:  5\n",
      "dataset size:  48447\n",
      "level:  6\n",
      "dataset size:  25375\n",
      "level:  6\n",
      "dataset size:  23071\n",
      "kdnodes:  2\n",
      "kdnodes:  3\n",
      "kdnodes:  5\n",
      "kdnodes:  9\n",
      "level:  2\n",
      "dataset size:  323826\n",
      "level:  3\n",
      "dataset size:  161913\n",
      "level:  4\n",
      "dataset size:  80956\n",
      "level:  5\n",
      "dataset size:  40478\n",
      "level:  6\n",
      "dataset size:  20239\n",
      "level:  6\n",
      "dataset size:  20238\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  40477\n",
      "level:  6\n",
      "dataset size:  23729\n",
      "level:  6\n",
      "dataset size:  16747\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  4\n",
      "dataset size:  80956\n",
      "level:  5\n",
      "dataset size:  40478\n",
      "level:  6\n",
      "dataset size:  20239\n",
      "level:  6\n",
      "dataset size:  20238\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  40477\n",
      "level:  6\n",
      "dataset size:  20238\n",
      "level:  6\n",
      "dataset size:  20238\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "kdnodes:  8\n",
      "level:  3\n",
      "dataset size:  161912\n",
      "level:  4\n",
      "dataset size:  109099\n",
      "level:  5\n",
      "dataset size:  54549\n",
      "level:  6\n",
      "dataset size:  27274\n",
      "level:  6\n",
      "dataset size:  27274\n",
      "kdnodes:  2\n",
      "level:  5\n",
      "dataset size:  54549\n",
      "level:  6\n",
      "dataset size:  27274\n",
      "level:  6\n",
      "dataset size:  27274\n",
      "kdnodes:  2\n",
      "kdnodes:  4\n",
      "level:  4\n",
      "dataset size:  52812\n",
      "level:  5\n",
      "dataset size:  8787\n",
      "level:  5\n",
      "dataset size:  44024\n",
      "level:  6\n",
      "dataset size:  19897\n",
      "level:  6\n",
      "dataset size:  24126\n",
      "kdnodes:  2\n",
      "kdnodes:  3\n",
      "kdnodes:  7\n",
      "kdnodes:  15\n",
      "kdnodes:  24\n",
      "kdnodes:  61\n",
      "61\n",
      "[[array([[-180,   -5],\n",
      "       [ -90,  -76]]), 4775], [array([[-180,  -26],\n",
      "       [ -76,  -53]]), 31844], [array([[-26, -23],\n",
      "       [-76, -53]]), 5685], [array([[-180,  -25],\n",
      "       [ -53,  -32]]), 18765], [array([[-25, -23],\n",
      "       [-53, -32]]), 18764], [array([[-23, -21],\n",
      "       [-76, -43]]), 18765], [array([[-21,  -5],\n",
      "       [-76, -43]]), 18764], [array([[-23, -14],\n",
      "       [-43, -32]]), 18765], [array([[-14,  -5],\n",
      "       [-43, -32]]), 18764], [array([[ -5,  19],\n",
      "       [-90, -99]]), 18898], [array([[ 19,  30],\n",
      "       [-90, -99]]), 18898], [array([[ -5,   0],\n",
      "       [-99, -86]]), 9], [array([[  0,  30],\n",
      "       [-99, -96]]), 18893], [array([[  0,  30],\n",
      "       [-96, -86]]), 18893], [array([[ 30,  32],\n",
      "       [-90, -86]]), 1853], [array([[ -5,   1],\n",
      "       [-86, -76]]), 1927], [array([[ -5,   1],\n",
      "       [-76, -32]]), 30295], [array([[  1,  32],\n",
      "       [-86, -79]]), 22612], [array([[  1,  32],\n",
      "       [-79, -32]]), 22611], [array([[-180,   -6],\n",
      "       [ -32,  106]]), 16265], [array([[-180,   -6],\n",
      "       [ 106,  107]]), 16264], [array([[ -6,   1],\n",
      "       [-32, 104]]), 16264], [array([[ -6,   1],\n",
      "       [104, 107]]), 16264], [array([[-180,   -7],\n",
      "       [ 107,  112]]), 16264], [array([[-180,   -7],\n",
      "       [ 112,   90]]), 16264], [array([[ -7,   1],\n",
      "       [107, 110]]), 16264], [array([[ -7,   1],\n",
      "       [110,  90]]), 16264], [array([[  1,  24],\n",
      "       [-32,  98]]), 16880], [array([[ 24,  30],\n",
      "       [-32,  98]]), 16880], [array([[  1,  13],\n",
      "       [ 98, 100]]), 16880], [array([[ 13,  30],\n",
      "       [ 98, 100]]), 16880], [array([[ 30,  32],\n",
      "       [-32, 100]]), 1908], [array([[  1,   3],\n",
      "       [100, 102]]), 16777], [array([[  3,  30],\n",
      "       [100, 102]]), 16777], [array([[  1,  14],\n",
      "       [102,  90]]), 16777], [array([[ 14,  30],\n",
      "       [102,  90]]), 16776], [array([[ 30,  32],\n",
      "       [100,  90]]), 2321], [array([[ 32,  35],\n",
      "       [-90, -84]]), 31869], [array([[ 35,  40],\n",
      "       [-90, -84]]), 31869], [array([[ 32,  38],\n",
      "       [-84,   6]]), 31869], [array([[ 38,  40],\n",
      "       [-84,   6]]), 31868], [array([[ 40,  42],\n",
      "       [-90, -76]]), 22693], [array([[ 42, 180],\n",
      "       [-90, -76]]), 29053], [array([[ 40,  42],\n",
      "       [-76,   6]]), 27282], [array([[ 42, 180],\n",
      "       [-76,   0]]), 25375], [array([[ 42, 180],\n",
      "       [  0,   6]]), 23071], [array([[ 32,  37],\n",
      "       [  6, 126]]), 20239], [array([[ 32,  37],\n",
      "       [126, 134]]), 20238], [array([[37, 40],\n",
      "       [ 6, 31]]), 23729], [array([[ 37,  40],\n",
      "       [ 31, 134]]), 16747], [array([[ 32,  35],\n",
      "       [134, 137]]), 20239], [array([[ 32,  35],\n",
      "       [137,  90]]), 20238], [array([[ 35,  40],\n",
      "       [134, 139]]), 20238], [array([[ 35,  40],\n",
      "       [139,  90]]), 20238], [array([[40, 41],\n",
      "       [ 6, 29]]), 27274], [array([[40, 41],\n",
      "       [29, 31]]), 27274], [array([[ 41, 180],\n",
      "       [  6,  19]]), 27274], [array([[ 41, 180],\n",
      "       [ 19,  31]]), 27274], [array([[40, 42],\n",
      "       [31, 90]]), 8787], [array([[ 42, 180],\n",
      "       [ 31,  38]]), 19897], [array([[ 42, 180],\n",
      "       [ 38,  90]]), 24126]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "query_bound = [query_bound_dim1,query_bound_dim2]\n",
    "Dimorder = [0,1]\n",
    "domains = [[-180,180],[-90,90]]\n",
    "kdnodes = LearnedKDTree(dataset, query_bound, Dimorder, domains, threshold=32000)\n",
    "print(len(kdnodes))\n",
    "print(kdnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-180, -5, -90, -76, 4775], [-180, -26, -76, -53, 31844], [-26, -23, -76, -53, 5685], [-180, -25, -53, -32, 18765], [-25, -23, -53, -32, 18764], [-23, -21, -76, -43, 18765], [-21, -5, -76, -43, 18764], [-23, -14, -43, -32, 18765], [-14, -5, -43, -32, 18764], [-5, 19, -90, -99, 18898], [19, 30, -90, -99, 18898], [-5, 0, -99, -86, 9], [0, 30, -99, -96, 18893], [0, 30, -96, -86, 18893], [30, 32, -90, -86, 1853], [-5, 1, -86, -76, 1927], [-5, 1, -76, -32, 30295], [1, 32, -86, -79, 22612], [1, 32, -79, -32, 22611], [-180, -6, -32, 106, 16265], [-180, -6, 106, 107, 16264], [-6, 1, -32, 104, 16264], [-6, 1, 104, 107, 16264], [-180, -7, 107, 112, 16264], [-180, -7, 112, 90, 16264], [-7, 1, 107, 110, 16264], [-7, 1, 110, 90, 16264], [1, 24, -32, 98, 16880], [24, 30, -32, 98, 16880], [1, 13, 98, 100, 16880], [13, 30, 98, 100, 16880], [30, 32, -32, 100, 1908], [1, 3, 100, 102, 16777], [3, 30, 100, 102, 16777], [1, 14, 102, 90, 16777], [14, 30, 102, 90, 16776], [30, 32, 100, 90, 2321], [32, 35, -90, -84, 31869], [35, 40, -90, -84, 31869], [32, 38, -84, 6, 31869], [38, 40, -84, 6, 31868], [40, 42, -90, -76, 22693], [42, 180, -90, -76, 29053], [40, 42, -76, 6, 27282], [42, 180, -76, 0, 25375], [42, 180, 0, 6, 23071], [32, 37, 6, 126, 20239], [32, 37, 126, 134, 20238], [37, 40, 6, 31, 23729], [37, 40, 31, 134, 16747], [32, 35, 134, 137, 20239], [32, 35, 137, 90, 20238], [35, 40, 134, 139, 20238], [35, 40, 139, 90, 20238], [40, 41, 6, 29, 27274], [40, 41, 29, 31, 27274], [41, 180, 6, 19, 27274], [41, 180, 19, 31, 27274], [40, 42, 31, 90, 8787], [42, 180, 31, 38, 19897], [42, 180, 38, 90, 24126]]\n"
     ]
    }
   ],
   "source": [
    "# transfer the form from [[(x,x),(x,x)],x] to [x,x,x,x,x], specified to 2D only!!!!!!\n",
    "nodes = []\n",
    "for i in range(len(kdnodes)):\n",
    "    each_node = [kdnodes[i][0][0][0], kdnodes[i][0][0][1],kdnodes[i][0][1][0],kdnodes[i][0][1][1],kdnodes[i][1]]\n",
    "    nodes.append(each_node)\n",
    "print(nodes)\n",
    "np.savetxt('/Users/lizhe/Desktop/LearnedKDTree/DataAndWorkload/SyntheticWorkload/KDnodes_Tweet_C10_P10_S5.csv',nodes,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,3],[2,3,4]]\n",
    "a = np.reshape(a,(1,-1))\n",
    "a = [a,9]\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
