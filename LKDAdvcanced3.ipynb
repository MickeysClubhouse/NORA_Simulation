{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb # this can only be installed from pip (no conda)\n",
    "from QueryTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import import_ipynb # this can only be installed from pip (no conda)\n",
    "# from LKD3 import *\n",
    "\n",
    "# = = = LKD Advanced Version 3 = = =\n",
    "\n",
    "def LKDAdvanced3(dataset, query, domains, threshold, level, current_dim = 0):\n",
    "\n",
    "    '''\n",
    "    This is the 3rd version of the LKD.\n",
    "    Main differences:\n",
    "      1. Retrieve query for the current domain. (compared with version 1)\n",
    "      2. Consider the #queries cancelled when calculating Min-Median-Distance. (compared with version 2)\n",
    "      \n",
    "    Parameters:\n",
    "      @dataset[i,k]: numpy object, i: ith record  k: kth dimension\n",
    "      @query[i,k,0/1]: numpy object, i: ith record  k: kth dimension  0/1: min/max \n",
    "      @domain[k][0/1]: numpy object, k: kth dimension  0/1: min/max\n",
    "      @threshold: the minimum number of records required in a partition\n",
    "      @level: the current level of the split (in KD-Tree), root is 0\n",
    "      @current_dim: the last split dimension\n",
    "      \n",
    "    Return:\n",
    "      @kdnodes[i][0/1][k][0/1]: array object,  i: ith record  0/1: domain/size  k: kth dimension  0/1: min/max \n",
    "    \n",
    "    '''\n",
    "    # check if the threshold is already satisfied\n",
    "    total_size = len(dataset)\n",
    "    \n",
    "    #print(\"level: \",level, \"  size: \", total_size)\n",
    "    if total_size <= 2*threshold:\n",
    "        kdnodes = []\n",
    "        kdnodes.append([domains, total_size])\n",
    "        return kdnodes\n",
    "    \n",
    "    # create query tree\n",
    "    query_trees = []\n",
    "    for i in range(len(domains)):\n",
    "        qtree = QueryTree(i)\n",
    "        qtree.loadQuerySetFromQueries(query)\n",
    "        qtree.buildQueryTree()\n",
    "        query_trees.append(qtree)\n",
    "        \n",
    "    split_from_median_position_tag = False\n",
    "    \n",
    "    removed_queries_each_dim = [0 for D in range(len(domains))] # newly added variable for version 3, initialized as 0\n",
    "    query_remove_cost_each_dim = [0 for D in range(len(domains))]\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # newly added variable for version 3, calculate the added cost, i.e., cost for execute the query\n",
    "        query_remove_cost_each_dim = [removed_queries_each_dim[D]*total_size for D in range(len(removed_queries_each_dim))]\n",
    "        \n",
    "        split_from_median_position_tag = False\n",
    "        \n",
    "        split_distance_each_dim = []\n",
    "        split_value_each_dim = []\n",
    "        caches = []\n",
    "        medians = []\n",
    "\n",
    "        # for each dimension, we calculated the distance from median to its first non-cross split\n",
    "        for D in range(len(domains)):\n",
    "\n",
    "            # median, with fast median algorithm\n",
    "            median = np.median(dataset[:,D]) # the median value\n",
    "            split_value = median # by default (i.e., without shift), is median\n",
    "\n",
    "            # split distance\n",
    "            median_shift_distance_lower = 0\n",
    "            median_shift_distance_upper = 0\n",
    "            min_median_distance = 0\n",
    "            \n",
    "            if len(query_trees[D].node_dict) == 0:\n",
    "                split_distance_each_dim.append(min_median_distance)\n",
    "                split_value_each_dim.append(split_value)\n",
    "                medians.append(median)\n",
    "                continue\n",
    "            \n",
    "            # check if the default split position intersect some query\n",
    "            is_overlap, cache = query_trees[D].queryValue(median)\n",
    "\n",
    "            # if overlap, find out the shift distance\n",
    "            if is_overlap:\n",
    "                overlap_query_lower = cache[1]\n",
    "                overlap_query_upper = cache[2]\n",
    "\n",
    "                # check if the 2 ends exceeds the current domain\n",
    "                if overlap_query_lower <= domains[D][0] and overlap_query_upper >= domains[D][1]: # if yes\n",
    "                    median_shift_distance_lower = int(total_size / 2)\n",
    "                    median_shift_distance_upper = int(total_size / 2)\n",
    "                    min_median_distance = int(total_size / 2)\n",
    "                else: # if not\n",
    "                    median_shift_distance_lower = len(dataset[(dataset[:,D]>=overlap_query_lower) & (dataset[:,D] < median)])\n",
    "                    median_shift_distance_upper = len(dataset[(dataset[:,D]<=overlap_query_upper) & (dataset[:,D] > median)])\n",
    "                    min_median_distance = min(median_shift_distance_lower, median_shift_distance_upper)\n",
    "                    if median_shift_distance_lower < median_shift_distance_upper:\n",
    "                        split_value = overlap_query_lower\n",
    "                    else:\n",
    "                        split_value = overlap_query_upper\n",
    "\n",
    "            # record the split shift (i.e., min median distance) and split value\n",
    "            split_distance_each_dim.append(min_median_distance)\n",
    "            split_value_each_dim.append(split_value)\n",
    "            caches.append(cache)\n",
    "            medians.append(median)\n",
    "\n",
    "        # aftern calculating the min median distance for each dimension\n",
    "        split_distance_each_dim = np.asarray(split_distance_each_dim)\n",
    "        split_dimension = 0\n",
    "        split_value = 0\n",
    "        \n",
    "        \n",
    "        # consider the split priority using added cost first\n",
    "        if max(query_remove_cost_each_dim) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            # has been through diveIn operation\n",
    "            successful_split_flag = False\n",
    "            split_order = np.argsort(query_remove_cost_each_dim)  # sort, ascending\n",
    "            for i in range(len(query_remove_cost_each_dim)):\n",
    "                split_dimension = np.where(split_order==i)[0][0] # get the ith smallest\n",
    "#                 print(split_dimension)\n",
    "                split_value = medians[split_dimension]\n",
    "                # if the partition is small and able to split, to avoid redundant reocrds that make it unsplitable by value\n",
    "                if total_size < 3*threshold and split_value_each_dim[split_dimension] == 0:\n",
    "                    # split from median directly\n",
    "                    split_from_median_position_tag = True\n",
    "                    break\n",
    "                # check if the subnodes greater than threshold\n",
    "                sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "                sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "                if len(sub_dataset1) < threshold or len(sub_dataset2) < threshold:\n",
    "                    continue \n",
    "                else:\n",
    "                    successful_split_flag = True\n",
    "                    break\n",
    "        \n",
    "        if split_from_median_position_tag:\n",
    "            break\n",
    "        \n",
    "        # Using Advanced Split\n",
    "        # if every dimension is not able to split!\n",
    "        if min(split_distance_each_dim) >= int((total_size / 2)-5):\n",
    "            for D in range(len(domains)):\n",
    "                removed_queries = query_trees[D].diveIn(caches[D][0], medians[D])\n",
    "                removed_queries_each_dim[D] += removed_queries  # newly added variable for version 3\n",
    "            continue\n",
    "        \n",
    "        # degradation mechansim (if every dimension is valid to split, then using round robin)\n",
    "        # the 5 here is an error tolerance\n",
    "        if max(split_distance_each_dim) <= 5: # this should have the 2 sub partitions above the threshold size\n",
    "            split_dimension = current_dim + 1\n",
    "            if split_dimension >= len(domains):\n",
    "                split_dimension %= len(domains)\n",
    "            split_value = np.median(dataset[:,split_dimension])\n",
    "            break # jump to the split\n",
    "        \n",
    "        # the normal case\n",
    "        successful_split_flag = False\n",
    "        split_order = np.argsort(split_distance_each_dim)  # sort, ascending\n",
    "        for i in range(len(split_distance_each_dim)):\n",
    "            split_dimension = np.where(split_order==i)[0][0] # get the ith smallest\n",
    "            split_value = split_value_each_dim[split_dimension]\n",
    "            # if the partition is small and able to split, to avoid redundant reocrds that make it unsplitable by value\n",
    "            if total_size < 3*threshold and split_value_each_dim[split_dimension] == 0:\n",
    "                # split from median directly\n",
    "                split_from_median_position_tag = True\n",
    "                break\n",
    "            # check if the subnodes greater than threshold\n",
    "            sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "            sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "            if len(sub_dataset1) < threshold or len(sub_dataset2) < threshold:\n",
    "                continue \n",
    "            else:\n",
    "                successful_split_flag = True\n",
    "                break\n",
    "        \n",
    "        if split_from_median_position_tag:\n",
    "            break\n",
    "        \n",
    "        # Using Advanced Split if none of the above split can create legal sub partitions\n",
    "        if successful_split_flag:\n",
    "            break # jump to the split\n",
    "        else: # Using Advanced Split\n",
    "            dive_count = 0\n",
    "            for D in range(len(domains)):\n",
    "                if len(caches[D]) != 0:\n",
    "                    removed_queries = query_trees[D].diveIn(caches[D][0], medians[D])\n",
    "                    removed_queries_each_dim[D] += removed_queries # newly added variable for version 3\n",
    "                    dive_count += 1\n",
    "            if dive_count == 0: # indicate none-of the split is OK\n",
    "                kdnodes = []\n",
    "                kdnodes.append([domains, total_size])\n",
    "                return kdnodes\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    sub_dataset1 = []\n",
    "    sub_dataset2 = []\n",
    "    if split_from_median_position_tag:\n",
    "        dataset = dataset[np.argsort((dataset[:,split_dimension]))]\n",
    "        sub_dataset1 = dataset[0:int(total_size/2)]\n",
    "        sub_dataset2 = dataset[int(total_size/2):-1]\n",
    "    else:\n",
    "        # split the dataset according to the split position\n",
    "        sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "        sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "\n",
    "    # change the domains\n",
    "    sub_domains1 = np.copy(domains)\n",
    "    sub_domains1[split_dimension][1] = split_value\n",
    "    sub_domains2 = np.copy(domains)\n",
    "    sub_domains2[split_dimension][0] = split_value\n",
    "\n",
    "    # filter the queries for each sub node\n",
    "    sub_query1 = query[query[:,split_dimension,0] < split_value]\n",
    "    sub_query2 = query[query[:,split_dimension,1] > split_value]\n",
    "    \n",
    "    # used to see the current depth\n",
    "    level += 1\n",
    "\n",
    "    # recursion\n",
    "    kdnodes = []\n",
    "    kdnodes.extend(LKDAdvanced3(sub_dataset1, sub_query1, sub_domains1, threshold, level, split_dimension))\n",
    "    kdnodes.extend(LKDAdvanced3(sub_dataset2, sub_query2, sub_domains2, threshold, level, split_dimension))\n",
    "    \n",
    "#     if level == 1:\n",
    "#         print(len(query_trees[0].primary_nodes))\n",
    "#         print(len(query_trees[0].node_dict))\n",
    "#         print(len(query_trees[1].primary_nodes))\n",
    "#         print(len(query_trees[1].node_dict))\n",
    "    return kdnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
