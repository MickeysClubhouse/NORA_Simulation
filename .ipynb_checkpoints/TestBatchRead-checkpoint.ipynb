{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import import_ipynb\n",
    "import time\n",
    "from numpy import genfromtxt\n",
    "import rtree\n",
    "from rtree import index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - 0 - - - \n",
      "load time:  0.7495675086975098\n",
      "- - - 1 - - - \n",
      "load time:  0.8139548301696777\n",
      "- - - 2 - - - \n",
      "load time:  0.855985164642334\n",
      "- - - 3 - - - \n",
      "load time:  0.9320805072784424\n",
      "- - - 4 - - - \n",
      "load time:  1.012078046798706\n",
      "- - - 5 - - - \n",
      "load time:  1.1780333518981934\n",
      "- - - 6 - - - \n",
      "load time:  1.167330026626587\n",
      "- - - 7 - - - \n",
      "load time:  1.2551357746124268\n",
      "- - - 8 - - - \n",
      "load time:  1.4044301509857178\n",
      "- - - 9 - - - \n",
      "load time:  1.4435057640075684\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100000\n",
    "TOTAL_SIZE = 100000\n",
    "for i in range(10):\n",
    "    s_time = time.time()\n",
    "    dataset = genfromtxt('C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Legacy/data/TPCH_12M_8Field.csv', delimiter=',',\n",
    "                        skip_header = i*batch_size, max_rows = batch_size)\n",
    "    e_time = time.time()\n",
    "    print('- - -',i,'- - - ')\n",
    "    print('load time: ', e_time - s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conclusion: numpy method is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process time:  1.2424235343933105\n",
      "1000000\n",
      "process time:  1.235649585723877\n",
      "1000000\n",
      "process time:  1.2551043033599854\n",
      "1000000\n",
      "process time:  1.2444002628326416\n",
      "1000000\n",
      "process time:  1.1985268592834473\n",
      "1000000\n",
      "process time:  1.1965761184692383\n",
      "1000000\n",
      "process time:  1.2083215713500977\n",
      "1000000\n",
      "process time:  1.1897449493408203\n",
      "1000000\n",
      "process time:  1.1985223293304443\n",
      "1000000\n",
      "process time:  1.1819350719451904\n",
      "1000000\n",
      "process time:  1.2297420501708984\n",
      "1000000\n",
      "process time:  1.2746565341949463\n",
      "997995\n"
     ]
    }
   ],
   "source": [
    "last_time = time.time()\n",
    "for chunk_df in pd.read_csv('C:/Users/Cloud/iCloudDrive/HUAWEI_LKD/Dataset/Legacy/data/TPCH_12M_8Field.csv', chunksize=1000000):\n",
    "    current_time = time.time()\n",
    "    print('process time: ',current_time - last_time)\n",
    "    print(len(chunk_df))\n",
    "    last_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conclusion: pandas method for chunk processing is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total process time:  0.1064152717590332\n"
     ]
    }
   ],
   "source": [
    "# begin_time = time.time()\n",
    "\n",
    "# partition_path = 'D:/iCloudDrive/HUAWEI_LKD/HDFSExperiment/generated_partitions/nora_partitions'\n",
    "# partitions = load_partitions_from_file(partition_path)\n",
    "\n",
    "# p = index.Property()\n",
    "# p.leaf_capacity = 32 # cannot be less than 100, indicate the maximum capacity\n",
    "# p.index_capacity = 32\n",
    "# p.NearMinimumOverlapFactor = 16\n",
    "# p.fill_factor = 0.8\n",
    "# p.overwrite = True\n",
    "\n",
    "# pd_dict={}\n",
    "\n",
    "# # create index for the kdnodes\n",
    "# pidx = index.Index(properties = p) # Rtree index\n",
    "# for i in range(len(partitions)):\n",
    "#     #pidx.insert(partitions[i][-4], kdnode_2_border(partitions[i]))\n",
    "#     pidx.insert(i, kdnode_2_border(partitions[i]))\n",
    "    \n",
    "# end_time = time.time()   \n",
    "# print('total process time: ',end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try batch read tbl format\n",
    "import pandas as pd\n",
    "\n",
    "def kdnode_2_border(kdnode):\n",
    "    lower = [domain[0] for domain in kdnode[0]]\n",
    "    upper = [domain[1] for domain in kdnode[0]]\n",
    "    border = tuple(lower + upper) # non interleave\n",
    "    return border\n",
    "\n",
    "table_path = 'D:/iCloudDrive/HUAWEI_LKD/9a84f6cd-727f-4f10-ae95-10a0214e10a4-tpc-h-tool/2.18.0_rc2/dbgen/lineitem.tbl'\n",
    "\n",
    "col_names = ['_c'+str(i) for i in range(16)] # using the last column as pid\n",
    "cols = [i for i in range(16)]\n",
    "\n",
    "partition_path = 'D:/iCloudDrive/HUAWEI_LKD/HDFSExperiment/generated_partitions/qd_tree_partitions'\n",
    "# partition_path = '~/PartitionLayout/qd_tree_partitions'\n",
    "partitions = load_partitions_from_file(partition_path)\n",
    "\n",
    "p = index.Property()\n",
    "p.leaf_capacity = 32 # cannot be less than 100, indicate the maximum capacity\n",
    "p.index_capacity = 32\n",
    "p.NearMinimumOverlapFactor = 16\n",
    "p.fill_factor = 0.8\n",
    "p.overwrite = True\n",
    "\n",
    "pd_dict={}\n",
    "\n",
    "# create index for the kdnodes\n",
    "pidx = index.Index(properties = p) # Rtree index\n",
    "for i in range(len(partitions)):\n",
    "    #pidx.insert(partitions[i][-4], kdnode_2_border(partitions[i]))\n",
    "    pidx.insert(i, kdnode_2_border(partitions[i]))\n",
    "    \n",
    "def find_pid(row, used_dims):\n",
    "    row_used_dims = row[used_dims]\n",
    "    #print('row_used_dims: ',row_used_dims)\n",
    "    row_list = row_used_dims.values.tolist()\n",
    "    #print('row_list: ', row_list)\n",
    "    border = tuple(row_list+row_list)\n",
    "    #print('border: ',border)\n",
    "    overlap_pid = list(pidx.intersection(border))\n",
    "    return overlap_pid[0]\n",
    "\n",
    "def find_pid2(row):\n",
    "    #print('row: ', row)\n",
    "    row_list = row.values.tolist()\n",
    "    border = tuple(row_list+row_list)\n",
    "    #print('border: ',border)\n",
    "    return list(pidx.intersection(border))[0]\n",
    "\n",
    "def find_pid3(row, used_dims):\n",
    "    row_used_dims = row[used_dims]\n",
    "    row_list = row_used_dims.values.tolist()\n",
    "    border = tuple(row_list+row_list)\n",
    "    pid = list(pidx.intersection(border))[0]\n",
    "    if pid in pd_dict:\n",
    "        pd_dict[pid].loc[len(pd_dict[pid])] = row.values.tolist() #pd_dict[pid].append(row, ignore_index=True)\n",
    "    else:\n",
    "        pd_dict.update({pid: pd.DataFrame(columns=col_names)})\n",
    "        pd_dict[pid].loc[0] = row.values.tolist()\n",
    "\n",
    "def find_pid4(row, used_dims):\n",
    "    row_used_dims = row[used_dims]\n",
    "    row_list = row_used_dims.values.tolist()\n",
    "    border = tuple(row_list+row_list)\n",
    "    pid = list(pidx.intersection(border))[0]\n",
    "    if pid in pd_dict:\n",
    "        pd_dict[pid].append(row.values.tolist())  #pd_dict[pid].append(row, ignore_index=True)\n",
    "    else:\n",
    "        pd_dict.update({pid: []})\n",
    "        pd_dict[pid].append(row.values.tolist())\n",
    "        \n",
    "def find_pid5(row, used_dims, pidx):\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_list = row_numpy[used_dims].tolist()\n",
    "    border = tuple(row_list+row_list)\n",
    "    pid = list(pidx.intersection(border))[0]\n",
    "    if pid in pd_dict:\n",
    "        pd_dict[pid].append(row_numpy.tolist())  #pd_dict[pid].append(row, ignore_index=True)\n",
    "    else:\n",
    "        pd_dict.update({pid: []})\n",
    "        pd_dict[pid].append(row_numpy.tolist())\n",
    "        #print(pd_dict[pid])\n",
    "        \n",
    "def find_pid6(row, used_dims):\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_list = row_numpy[used_dims].tolist()\n",
    "    border = tuple(row_list+row_list)\n",
    "    pid = list(pidx.intersection(border))[0]\n",
    "    if pid in pd_dict:\n",
    "        np.vstack((pd_dict[pid], row_numpy))  #pd_dict[pid].append(row, ignore_index=True)\n",
    "    else:\n",
    "        pd_dict.update({pid: row_numpy})\n",
    "        \n",
    "used_dims = [1,2]\n",
    "\n",
    "# begin_time = time.time()\n",
    "# last_time = time.time()\n",
    "\n",
    "# select_col_names = ['_c1','_c2']\n",
    "\n",
    "# count = 0\n",
    "# for chunk in pandas.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=10):\n",
    "#     print('chunk id ', count)\n",
    "#     #print(chunk.iloc[:,used_dims])\n",
    "#     print(chunk[select_col_names])\n",
    "#     #chunk['pid'] = chunk.apply(lambda row: find_pid(row, used_dims), axis = 1) \n",
    "#     #chunk['pid'] = chunk.iloc[:,1:3].apply(find_pid2) the vectorization operation can only support 1 dimension\n",
    "#     count += 1\n",
    "#     current_time = time.time()\n",
    "#     print('chunk process time: ',current_time - last_time)\n",
    "#     last_time = current_time\n",
    "#     if count > 0:\n",
    "#         break\n",
    "        \n",
    "# end_time = time.time()   \n",
    "# print('total process time: ',end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin_time = time.time()\n",
    "# pd_dict = {}\n",
    "# for i in range(1000):\n",
    "#     pd_dict.update({i:[]})\n",
    "# end_time = time.time()   \n",
    "# print('total process time: ',end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pid_data_dict = {}\n",
    "for k in [9, 12, 16, 0, 36, 32, 1, 24, 20, 41, 28, 17, 44, 4, 52, 8, 48]:\n",
    "    pid_data_dict.update({k:[]})\n",
    "    \n",
    "def process_chunk_row(row, used_dims, pidx):\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_used_dims_list = row_numpy[used_dims].tolist()\n",
    "    row_border = tuple(row_used_dims_list+row_used_dims_list)\n",
    "    pid = list(pidx.intersection(row_border))[0]\n",
    "    pid_data_dict[pid].append(row_numpy.tolist())\n",
    "#     if pid in pd_dict:\n",
    "#         pd_dict[pid].append(row_numpy.tolist())  #pd_dict[pid].append(row, ignore_index=True)\n",
    "#     else:\n",
    "#         pd_dict.update({pid: []})\n",
    "#         pd_dict[pid].append(row_numpy.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total process time:  3.4892258644104004\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "# 1.49 s ± 32.8 ms\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "count = 0\n",
    "for chunk in pandas.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=100000):\n",
    "    #print('chunk id ', count)\n",
    "    # %lprun -f find_pid chunk.apply(lambda row: find_pid(row, used_dims), axis = 1)\n",
    "    #chunk['pid'] = chunk.apply(lambda row: find_pid(row, used_dims), axis = 1)\n",
    "    \n",
    "    #chunk.apply(lambda row: find_pid3(row, used_dims), axis = 1) # totally 44s for 1W rows\n",
    "    #%lprun -f find_pid3 chunk.apply(lambda row: find_pid3(row, used_dims), axis = 1)\n",
    "    \n",
    "    # chunk.apply(lambda row: find_pid4(row, used_dims), axis = 1)\n",
    "    #%lprun -f find_pid4 chunk.apply(lambda row: find_pid4(row, used_dims), axis = 1) \n",
    "    \n",
    "#     chunk.apply(lambda row: find_pid5(row, used_dims, pidx), axis = 1)\n",
    "#     chunk.apply(lambda row: find_pid6(row, used_dims), axis = 1)\n",
    "\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx), axis = 1)\n",
    "    \n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break \n",
    "        \n",
    "end_time = time.time()   \n",
    "print('total process time: ',end_time - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-206-458e5643a302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocess_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, pidx), axis = 1)\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "count = 0\n",
    "for chunk in pandas.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=100000):\n",
    "\n",
    "    p = Process(target=process_chunk, args=(chunk,))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    \n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break \n",
    "        \n",
    "end_time = time.time()   \n",
    "print('total process time: ',end_time - begin_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: [], 12: [], 16: [], 0: [], 36: [], 32: [], 1: [], 24: [], 20: [], 41: [], 28: [], 17: [], 44: [], 4: [], 52: [], 8: [], 48: []}\n"
     ]
    }
   ],
   "source": [
    "print(pid_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-0643258d1a6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 9"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pd_dict[9], columns=col_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([9, 12, 16, 0, 36, 32, 1, 24, 20, 41, 28, 17, 44, 4, 52, 8, 48])\n"
     ]
    }
   ],
   "source": [
    "print(pd_dict.keys())\n",
    "# print(pd_dict[12])\n",
    "# print(len(pd_dict[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: [], 12: [], 16: [], 0: [], 36: [], 32: [], 1: [], 24: [], 20: [], 41: [], 28: [], 17: [], 44: [], 4: [], 52: [], 8: [], 48: []}\n"
     ]
    }
   ],
   "source": [
    "for pid in pd_dict.keys():\n",
    "    pd_dict[pid] = []\n",
    "print(pd_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_1+2thread_2+1thread_1+3thread_2+2{1: 5, 2: 3}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check concurrent write on dict\n",
    "import _thread\n",
    "\n",
    "dict_pid = {1:0, 2:0}\n",
    "\n",
    "def update_dict(key, val, name):\n",
    "    dict_pid[key] += val\n",
    "    print(name)\n",
    "\n",
    "_thread.start_new_thread(update_dict, (1, 2, \"thread_1+2\") )\n",
    "_thread.start_new_thread(update_dict, (2, 2, \"thread_2+2\") )\n",
    "_thread.start_new_thread(update_dict, (2, 1, \"thread_2+1\") )\n",
    "_thread.start_new_thread(update_dict, (1, 3, \"thread_1+3\") )\n",
    "\n",
    "print(dict_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "dict_pid = {1:0, 2:0}\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, tid, name, dict_pid, key, value):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.tid = tid\n",
    "        self.name = name\n",
    "        self.dict_pid = dict_pid\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        \n",
    "    def run(self):\n",
    "        self.dict_pid[self.key] += self.value\n",
    "\n",
    "thread1 = myThread(1, \"Thread-1\", dict_pid, 1, 2)\n",
    "thread2 = myThread(2, \"Thread-2\", dict_pid, 2, 2)\n",
    "thread3 = myThread(3, \"Thread-3\", dict_pid, 2, 1)\n",
    "thread4 = myThread(4, \"Thread-4\", dict_pid, 1, 3)\n",
    "\n",
    "\n",
    "# 开启新线程\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "thread4.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 5, 2: 3}\n"
     ]
    }
   ],
   "source": [
    "print(dict_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = a[2:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "dict = {1:2, 2:4}\n",
    "print(list(dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mem_top())\n",
    "test_arr = []\n",
    "for i in range(10000000):\n",
    "    test_arr.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
