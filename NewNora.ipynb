{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartitionNode:\n",
    "    '''\n",
    "    A partition node, including both the internal and leaf nodes in the partition tree\n",
    "    '''\n",
    "    def __init__(self, num_dims = 0, boundary = [], nid = None, pid = None, is_irregular_shape_parent = False,\n",
    "                 is_irregular_shape = False, num_children = 0, children_ids = [], is_leaf = True, node_size = 0):\n",
    "        \n",
    "        self.num_dims = num_dims # number of dimensions\n",
    "        # the domain, [l1,l2,..,ln, u1,u2,..,un,], for irregular shape partition, one need to exempt its siblings\n",
    "        self.boundary = boundary # I think the lower side should be inclusive and the upper side should be exclusive?\n",
    "        self.nid = nid # node id\n",
    "        self.pid = pid # parent id\n",
    "        self.is_irregular_shape_parent = is_irregular_shape_parent # whether the [last] child is an irregular shape partition\n",
    "        self.is_irregular_shape = is_irregular_shape # an irregular shape partition cannot be further split, and it must be a leaf node\n",
    "        self.num_children = num_children # number of children, should be 0, 2, or 3\n",
    "        self.children_ids = children_ids # if it's the irregular shape parent, then the last child should be the irregular partition\n",
    "        self.is_leaf = is_leaf\n",
    "        self.node_size = node_size # number of records in this partition\n",
    "        \n",
    "        self.dataset = None # only used in partition algorithms, tempoary, should consist records that within this partition\n",
    "        self.queryset = None # only used in partition algorithms, tempoary, should consist queries that overlap this partition\n",
    "        \n",
    "    def is_overlap(self, query):\n",
    "        '''\n",
    "        query is in plain form, i.e., [l1,l2,...,ln, u1,u2,...,un]\n",
    "        !query dimension should match the partition dimensions! i.e., all projected or all not projected\n",
    "        return 0 if no overlap\n",
    "        return 1 if overlap\n",
    "        return 2 if inside\n",
    "        '''\n",
    "        if len(query) != 2 * self.num_dims:\n",
    "            return -1 # error\n",
    "        \n",
    "        overlap_flag = True\n",
    "        inside_flag = True\n",
    "        \n",
    "        for i in range(self.num_dims):\n",
    "            if query[i] > self.boundary[self.num_dims + i] or query[self.num_dims + i] < self.boundary[i]:\n",
    "                overlap_flag = False\n",
    "                inside_flag = False\n",
    "                return 0\n",
    "            elif query[i] < self.boundary[i] or query[self.num_dims + i] > self.boundary[self.num_dims + i]:\n",
    "                inside_flag = False\n",
    "                \n",
    "        if inside_flag:\n",
    "            return 2\n",
    "        elif overlap_flag:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def is_overlap_np(self, query):\n",
    "        '''\n",
    "        the numpy version of the is_overlap function\n",
    "        the query here and boundary class attribute should in the form of numpy array\n",
    "        '''\n",
    "        if all((boundary[0:self.num_dims] > query[self.num_dims:]) | (boundary[self.num_dims:] < query[0:self.num_dims])):\n",
    "            return 0 # no overlap\n",
    "        elif all((boundary[0:self.num_dims] >= query[0:self.num_dims]) & (boundary[self.num_dims:] <= query[self.num_dims:])):\n",
    "            return 2 # inside\n",
    "        else:\n",
    "            return 1 # overlap\n",
    "    \n",
    "    def get_candidate_cuts(self, extended = False):\n",
    "        '''\n",
    "        get the candidate cut positions\n",
    "        if extended is set to True, also add medians from all dimensions\n",
    "        '''\n",
    "        candidate_cut_pos = []\n",
    "        for query in self.queryset:\n",
    "            for dim in range(self.num_dims):\n",
    "                # check if the cut position is inside the partition, as the queryset are queries overlap this partition\n",
    "                if query[dim] >= self.boundary[dim] and query[dim] <= self.boundary[self.num_dims+dim]:\n",
    "                    candidate_cut_pos.append((dim, query[dim]))\n",
    "                if query[self.num_dims+dim] >= self.boundary[dim] and query[self.num_dims+dim] <= self.boundary[self.num_dims+dim]:\n",
    "                    candidate_cut_pos.append((dim, query[self.num_dims+dim]))\n",
    "        \n",
    "        if extended:\n",
    "            for dim in range(self.num_dims):\n",
    "                split_value = np.median(self.dataset[:,dim])\n",
    "                candidate_cut_pos.append((dim, split_value))\n",
    "        \n",
    "        return candidate_cut_pos\n",
    "    \n",
    "    def if_split(self, split_dim, split_value, data_threshold): # rename: if_split_get_gain\n",
    "        '''\n",
    "        return the skip gain and children partition size if split a node from a given split dimension and split value\n",
    "        '''\n",
    "        #print(\"current_node.nid:\", current_node.nid)\n",
    "        #print(\"current_node.is_leaf:\", current_node.is_leaf)\n",
    "        #print(\"current_node.dataset is None:\", current_node.dataset is None)\n",
    "        sub_dataset1_size = np.count_nonzero(self.dataset[:,split_dim] < split_value) # process time: 0.007\n",
    "        sub_dataset2_size = self.node_size - sub_dataset1_size\n",
    "\n",
    "        if sub_dataset1_size < data_threshold or sub_dataset2_size < data_threshold:\n",
    "            return 0, sub_dataset1_size, sub_dataset2_size\n",
    "        \n",
    "        left_part, right_part, mid_part = self.split_queryset(split_dim, split_value)\n",
    "        num_overlap_child1 = len(left_part) + len(mid_part)\n",
    "        num_overlap_child2 = len(right_part) + len(mid_part)\n",
    "        \n",
    "        # temp_child_node1, temp_child_node2 = self.__if_split_get_child(split_dim, split_value)\n",
    "        skip_gain = len(self.queryset)*self.node_size - num_overlap_child1*sub_dataset1_size - num_overlap_child2*sub_dataset2_size\n",
    "        return skip_gain, sub_dataset1_size, sub_dataset2_size\n",
    "    \n",
    "#         # calculate skip gain\n",
    "#         num_overlap_current, num_overlap_child1, num_overlap_child2 = 0, 0, 0\n",
    "#         for query in self.queryset:\n",
    "#             if self.is_overlap(query) > 0:\n",
    "#                 num_overlap_current += 1\n",
    "#             if temp_child_node1.is_overlap(query) > 0:\n",
    "#                 num_overlap_child1 += 1\n",
    "#             if temp_child_node2.is_overlap(query) > 0:\n",
    "#                 num_overlap_child2 += 1\n",
    "#         skip_gain = num_overlap_current * self.node_size - num_overlap_child1 * sub_dataset1_size - num_overlap_child2 * sub_dataset2_size\n",
    "#         return skip_gain, sub_dataset1_size, sub_dataset2_size\n",
    "    \n",
    "    def if_bounding_split(self, data_threshold, approximate = False):\n",
    "        max_bound = self.__max_bound(self.queryset)\n",
    "        bound_size = self.query_result_size(max_bound, approximate)\n",
    "        if bound_size is None or bound_size < data_threshold: \n",
    "            return False, None\n",
    "        remaining_size = self.node_size - bound_size\n",
    "        if remaining_size < data_threshold:\n",
    "            return False, None\n",
    "        cost_before_split = len(self.queryset) * self.node_size\n",
    "        cost_bound_split = len(self.queryset) * bound_size\n",
    "        skip_gain = cost_before_split - cost_bound_split\n",
    "        return True, skip_gain\n",
    "    \n",
    "    def if_dual_bounding_split(self, split_dim, split_value, data_threshold, approximate = False):\n",
    "        '''\n",
    "        check whether it's available to perform dual bounding split\n",
    "        '''\n",
    "        # split queriese first\n",
    "        left_part, right_part, mid_part = self.split_queryset(split_dim, split_value)\n",
    "        max_bound_left = self.__max_bound(left_part)\n",
    "        max_bound_right = self.__max_bound(right_part)\n",
    "        \n",
    "        # Should we only consider the case when left and right cannot be further split? i.e., [b,2b)\n",
    "        # this check logic is given in the PartitionAlgorithm, not here, as the split action should be general\n",
    "        naive_left_size = np.count_nonzero(self.dataset[:,split_dim] < split_value)\n",
    "        naive_right_size = self.node_size - naive_left_size\n",
    "        \n",
    "        # get (irregular-shape) sub-partition size\n",
    "        left_size = self.query_result_size(max_bound_left, approximate)\n",
    "        if left_size is None: # there is no query within the left \n",
    "            left_size = naive_left_size # use the whole left part as its size\n",
    "        if left_size < data_threshold:\n",
    "            return False, None\n",
    "        right_size = self.query_result_size(max_bound_right, approximate)\n",
    "        if right_size is None: # there is no query within the right\n",
    "            right_size = naive_right_size # use the whole right part as its size\n",
    "        if right_size < data_threshold:\n",
    "            return False, None\n",
    "        remaining_size = self.node_size - left_size - right_size\n",
    "        if remaining_size < data_threshold:\n",
    "            return False, None\n",
    "        \n",
    "        # check cost\n",
    "        cost_before_split = len(self.queryset) * self.node_size\n",
    "        cost_dual_split = len(left_part) * left_size + len(right_part) * right_size + len(mid_part) * remaining_size\n",
    "        for query in mid_part:\n",
    "            # if it overlap left bounding box\n",
    "            if max_bound_left is None or self.__is_overlap(max_bound_left, query) > 0:\n",
    "                cost_dual_split += left_size\n",
    "            # if it overlap right bounding box\n",
    "            if max_bound_right is None or self.__is_overlap(max_bound_right, query) > 0:\n",
    "                cost_dual_split += right_size\n",
    "        skip_gain = cost_before_split - cost_dual_split\n",
    "        return True, skip_gain\n",
    "        \n",
    "    \n",
    "    def split_queryset(self, split_dim, split_value):\n",
    "        '''\n",
    "        split the queryset into 3 parts:\n",
    "        the left part, the right part, and those cross the split value\n",
    "        '''\n",
    "        if self.queryset is not None:\n",
    "            left_part = []\n",
    "            right_part = []\n",
    "            mid_part = []\n",
    "            for query in self.queryset:\n",
    "                if query[split_dim] >= split_value:\n",
    "                    right_part.append(query)\n",
    "                elif query[self.num_dims + split_dim] < split_value:\n",
    "                    left_part.append(query)\n",
    "                elif query[split_dim] < split_value and query[self.num_dims + split_dim] >= split_value:\n",
    "                    mid_part.append(query)\n",
    "            return left_part, right_part, mid_part\n",
    "        \n",
    "    def query_result_size(self, query, approximate = False):\n",
    "        '''\n",
    "        get the query result's size on this node\n",
    "        the approximate parameter is set to True, the use even distribution to approximate\n",
    "        '''\n",
    "        if query is None:\n",
    "            return None\n",
    "        \n",
    "        result_size = 0\n",
    "        if approximate:\n",
    "            query_volume = 1\n",
    "            volume = 1\n",
    "            for d in range(self.num_dims):\n",
    "                query_volume *= query[self.num_dims + d] - query[d]\n",
    "                volume *= self.boundary[self.num_dims + d] - self.boundary[d]\n",
    "\n",
    "            result_size = int(query_volume / volume * self.node_size)\n",
    "        else:\n",
    "            constraints = []\n",
    "            for d in range(self.num_dims):\n",
    "                constraint_L = dataset[:,d] >= query[d]\n",
    "                constraint_U = dataset[:,d] <= query[self.num_dims + d]\n",
    "                constraints.append(constraint_L)\n",
    "                constraints.append(constraint_U)\n",
    "            constraint = np.all(constraints, axis=0)\n",
    "            result_size = np.count_nonzero(constraint)\n",
    "        return result_size\n",
    "    \n",
    "    # = = = = = internal functions = = = = =\n",
    "    \n",
    "    def __is_overlap(self, boundary, query):\n",
    "        '''\n",
    "        the difference between this function and the public is_overlap function lies in the boundary parameter\n",
    "        '''\n",
    "        if len(query) != 2 * self.num_dims:\n",
    "            return -1 # error\n",
    "        \n",
    "        overlap_flag = True\n",
    "        inside_flag = True\n",
    "        \n",
    "        for i in range(self.num_dims):\n",
    "            if query[i] > boundary[self.num_dims + i] or query[self.num_dims + i] < boundary[i]:\n",
    "                overlap_flag = False\n",
    "                inside_flag = False\n",
    "                return 0\n",
    "            elif query[i] < boundary[i] or query[self.num_dims + i] > boundary[self.num_dims + i]:\n",
    "                inside_flag = False\n",
    "                \n",
    "        if inside_flag:\n",
    "            return 2\n",
    "        elif overlap_flag:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def __max_bound(self, queryset):\n",
    "        '''\n",
    "        bound the queries by their maximum bounding rectangle\n",
    "        then constraint the MBR by the node's boundary!\n",
    "        '''\n",
    "        if len(queryset) == 0:\n",
    "            return None\n",
    "        #if len(queryset) == 1:\n",
    "        #    pass, I don't think there will be shape issue here\n",
    "        \n",
    "        max_bound_L = np.amin(np.array(queryset)[:,0:self.num_dims],axis=0).tolist()\n",
    "        # bound the lower side with the boundary's lower side\n",
    "        max_bound_L = np.amax(np.array([max_bound_L, self.boundary[0:self.num_dims]]),axis=0).tolist()\n",
    "        \n",
    "        max_bound_U = np.amax(np.array(queryset)[:,self.num_dims:],axis=0).tolist()\n",
    "        # bound the upper side with the boundary's upper side\n",
    "        max_bound_U = np.amin(np.array([max_bound_U, self.boundary[self.num_dims:]]),axis=0).tolist()\n",
    "        \n",
    "        max_bound = max_bound_L + max_bound_U # concat\n",
    "        return max_bound\n",
    "    \n",
    "    def __if_split_get_child(self, split_dim, split_value): # should I rename this to if_split_get_child\n",
    "        '''\n",
    "        return 2 child nodes if a split take place on given dimension with given value\n",
    "        This function is only used to simplify the skip calculation process, it does not really split the node\n",
    "        '''\n",
    "        boundary1 = self.boundary.copy()\n",
    "        boundary1[split_dim + self.num_dims] = split_value\n",
    "        boundary2 = self.boundary.copy()\n",
    "        boundary2[split_dim] = split_value\n",
    "        child_node1 = PartitionNode(self.num_dims, boundary1)\n",
    "        child_node2 = PartitionNode(self.num_dims, boundary2)\n",
    "        return child_node1, child_node2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([False,False,True])\n",
    "# b = np.array([True,True,False])\n",
    "# a | b # array([ True,  True,  True])\n",
    "# all(a | b)\n",
    "\n",
    "# a = np.array([1,2,3])\n",
    "# b = np.array([2,3,3])\n",
    "# c = np.array([2,3,1])\n",
    "# a < b\n",
    "# a > c\n",
    "# (a < b) | (a > c)\n",
    "# # all(a < b)\n",
    "\n",
    "# a = np.array([[1,1,3,4],[2,2,1,1]])\n",
    "# np.amin(a,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    " class PartitionTree:\n",
    "        '''\n",
    "        The data structure that represent the partition layout, which also maintain the parent, children relation info\n",
    "        Designed to provide efficient online query and serialized ability\n",
    "        \n",
    "        The node data structure could be checked from the PartitionNode class\n",
    "        \n",
    "        '''   \n",
    "        def __init__(self, num_dims = 0, boundary = [], nid = 0, pid = -1, is_irregular_shape_parent = False,\n",
    "                     is_irregular_shape = False, num_children = 0, children_ids = [], is_leaf = True, node_size = 0):\n",
    "            \n",
    "            # the node id of root should be 0, its pid should be -1\n",
    "            self.pt_root = PartitionNode(num_dims, boundary, nid, pid, is_irregular_shape_parent, is_irregular_shape, \n",
    "                                    num_children, children_ids, is_leaf, node_size)\n",
    "            self.nid_node_dict = {nid: self.pt_root} # node id to node dictionary\n",
    "            self.node_count = 1 # the root node\n",
    "            \n",
    "        \n",
    "        # = = = = = public functions (API) = = = = =\n",
    "        \n",
    "        def save_tree(self, path):\n",
    "            node_list = self.__generate_node_list(self.pt_root) # do we really need this step?\n",
    "            serialized_node_list = self.__serialize(node_list)\n",
    "            np.savetxt(path, serialized_node_list, delimiter=',')\n",
    "            return serialized_node_list\n",
    "            \n",
    "        def load_tree(self, path):\n",
    "            serialized_node_list = genfromtxt(path, delimiter=',')\n",
    "            self.__build_tree_from_serialized_node_list(serialized_node_list)\n",
    "        \n",
    "        def query_single(self, query):\n",
    "            '''\n",
    "            query is in plain form, i.e., [l1,l2,...,ln, u1,u2,...,un]\n",
    "            return the overlapped leaf partitions ids!\n",
    "            '''\n",
    "            partition_ids = self.__find_overlapped_partition(self.pt_root, query)\n",
    "            return partition_ids\n",
    "        \n",
    "        def query_batch(self, queries):\n",
    "            '''\n",
    "            to be implemented\n",
    "            '''\n",
    "            pass\n",
    "        \n",
    "        def evaluate_query_cost(self, queries):\n",
    "            '''\n",
    "            get the logical IOs of the queris\n",
    "            '''\n",
    "            cost = 0\n",
    "            for query in queries:\n",
    "                overlapped_leaf_ids = self.query_single(query)\n",
    "                for nid in overlapped_leaf_ids:\n",
    "                    cost += self.nid_node_dict[nid].node_size\n",
    "            print(\"Total logical IOs:\", cost)\n",
    "            print(\"Average logical IOs:\", cost // len(queries))\n",
    "            return cost\n",
    "        \n",
    "        def add_node(self, parent_id, child_node):\n",
    "            child_node.nid = self.node_count\n",
    "            self.node_count += 1\n",
    "            \n",
    "            child_node.pid = parent_id\n",
    "            self.nid_node_dict[child_node.nid] = child_node\n",
    "            \n",
    "            self.nid_node_dict[parent_id].children_ids.append(child_node.nid)\n",
    "            self.nid_node_dict[parent_id].num_children += 1\n",
    "            self.nid_node_dict[parent_id].is_leaf = False\n",
    "        \n",
    "        \n",
    "        def apply_split(self, parent_nid, split_dim, split_value, split_type = 0):\n",
    "            '''\n",
    "            split_type = 0: split a node into 2 sub-nodes by a given dimension and value\n",
    "            split_type = 1: split a node by bounding split (will create an irregular shape partition)\n",
    "            split_type = 2: split a node by daul-bounding split (will create an irregular shape partition)\n",
    "            '''\n",
    "            parent_node = self.nid_node_dict[parent_nid]\n",
    "            \n",
    "            if split_type == 0:\n",
    "            \n",
    "                # create sub nodes\n",
    "                child_node1 = copy.deepcopy(parent_node)\n",
    "                child_node1.boundary[split_dim + child_node1.num_dims] = split_value\n",
    "                child_node1.children_ids = []\n",
    "\n",
    "                child_node2 = copy.deepcopy(parent_node)\n",
    "                child_node2.boundary[split_dim] = split_value\n",
    "                child_node2.children_ids = []\n",
    "\n",
    "                # if parent_node.dataset != None: # The truth value of an array with more than one element is ambiguous.\n",
    "                # https://stackoverflow.com/questions/36783921/valueerror-when-checking-if-variable-is-none-or-numpy-array\n",
    "                if parent_node.dataset is not None:\n",
    "                    child_node1.dataset = parent_node.dataset[parent_node.dataset[:,split_dim] < split_value]\n",
    "                    child_node1.node_size = len(child_node1.dataset)\n",
    "                    child_node2.dataset = parent_node.dataset[parent_node.dataset[:,split_dim] >= split_value]\n",
    "                    child_node2.node_size = len(child_node2.dataset)\n",
    "\n",
    "                if parent_node.queryset is not None:\n",
    "                    left_part, right_part, mid_part = parent_node.split_queryset(split_dim, split_value)\n",
    "                    child_node1.queryset = left_part + mid_part\n",
    "                    child_node2.queryset = right_part + mid_part\n",
    "\n",
    "                # update current node\n",
    "                self.add_node(parent_nid, child_node1)\n",
    "                self.add_node(parent_nid, child_node2)           \n",
    "            \n",
    "            elif split_type == 1: # must reach leaf node, hence no need to maintain dataset and queryset any more\n",
    "                \n",
    "                child_node1 = copy.deepcopy(parent_node) # the bounding partition\n",
    "                child_node2 = copy.deepcopy(parent_node) # the remaining partition, i.e., irregular shape\n",
    "                \n",
    "                child_node1.children_ids = []\n",
    "                child_node2.children_ids = []\n",
    "                \n",
    "                max_bound = parent_node._PartitionNode__max_bound(parent_node.queryset)\n",
    "                child_node1.boundary = max_bound\n",
    "                child_node2.is_irregular_shape = True\n",
    "                \n",
    "                bound_size = parent_node.query_result_size(max_bound, approximate = False)\n",
    "                remaining_size = parent_node.node_size - bound_size           \n",
    "                child_node1.node_size = bound_size\n",
    "                child_node2.node_size = remaining_size\n",
    "                \n",
    "                self.add_node(parent_nid, child_node1)\n",
    "                self.add_node(parent_nid, child_node2)\n",
    "                self.nid_node_dict[parent_nid].is_irregular_shape_parent = True\n",
    "            \n",
    "            elif split_type == 2: # must reach leaf node, hence no need to maintain dataset and queryset any more\n",
    "                \n",
    "                child_node1 = copy.deepcopy(parent_node) # the bounding partition 1\n",
    "                child_node2 = copy.deepcopy(parent_node) # the bounding partition 2\n",
    "                child_node3 = copy.deepcopy(parent_node) # the remaining partition, i.e., irregular shape\n",
    "                \n",
    "                child_node1.children_ids = []\n",
    "                child_node2.children_ids = []\n",
    "                child_node3.children_ids = []\n",
    "                \n",
    "                left_part, right_part, mid_part = parent_node.split_queryset(split_dim, split_value)\n",
    "                max_bound_1 = parent_node._PartitionNode__max_bound(left_part)\n",
    "                max_bound_2 = parent_node._PartitionNode__max_bound(right_part)\n",
    "                \n",
    "                child_node1.boundary = max_bound_1\n",
    "                child_node2.boundary = max_bound_2\n",
    "                child_node3.is_irregular_shape = True          \n",
    "                \n",
    "                # Should we only consider the case when left and right cannot be further split? i.e., [b,2b)\n",
    "                # this check logic is given in the PartitionAlgorithm, not here, as the split action should be general\n",
    "                naive_left_size = np.count_nonzero(parent_node.dataset[:,split_dim] < split_value)\n",
    "                naive_right_size = self.node_size - naive_left_size\n",
    "\n",
    "                # get (irregular-shape) sub-partition size\n",
    "                bound_size_1 = parent_node.query_result_size(max_bound_1, approximate)\n",
    "                if bound_size_1 is None: # there is no query within the left \n",
    "                    bound_size_1 = naive_left_size # use the whole left part as its size\n",
    "               \n",
    "                bound_size_2 = parent_node.query_result_size(max_bound_2, approximate)\n",
    "                if bound_size_2 is None: # there is no query within the right\n",
    "                    bound_size_2 = naive_right_size # use the whole right part as its size\n",
    "               \n",
    "                remaining_size = self.node_size - bound_size_1 - bound_size_2\n",
    "                \n",
    "                child_node1.node_size = bound_size_1\n",
    "                child_node2.node_size = bound_size_2\n",
    "                child_node3.node_size = remaining_size\n",
    "                \n",
    "                self.add_node(parent_nid, child_node1)\n",
    "                self.add_node(parent_nid, child_node2)\n",
    "                self.add_node(parent_nid, child_node3)\n",
    "                self.nid_node_dict[parent_nid].is_irregular_shape_parent = True\n",
    "            \n",
    "            else:\n",
    "                print(\"Invalid Split Type!\")\n",
    "            \n",
    "            del self.nid_node_dict[parent_nid].dataset\n",
    "            del self.nid_node_dict[parent_nid].queryset\n",
    "            #self.nid_node_dict[parent_nid] = parent_node\n",
    "            return child_node1, child_node2\n",
    "        \n",
    "        def get_leaves(self):\n",
    "            nodes = []\n",
    "            for nid, node in self.nid_node_dict.items():\n",
    "                if node.is_leaf:\n",
    "                    nodes.append(node)\n",
    "            return nodes\n",
    "        \n",
    "        def visualize(self, dims = [0, 1], queries = []):\n",
    "            '''\n",
    "            visualize the partition tree's leaf nodes\n",
    "            '''\n",
    "            if len(dims) == 2:\n",
    "                self.__visualize_2d(dims, queries)\n",
    "            else:\n",
    "                self.__visualize_3d(dims[0:3], queries)\n",
    "            \n",
    "        \n",
    "        # = = = = = internal functions = = = = =\n",
    "        \n",
    "        def __generate_node_list(self, node):\n",
    "            '''\n",
    "            recursively add childrens into the list\n",
    "            '''\n",
    "            node_list = [node]\n",
    "            for nid in node.children_ids:\n",
    "                node_list += self.__generate_node_list(self.nid_node_dict[nid])\n",
    "            return node_list\n",
    "        \n",
    "        def __serialize(self, node_list):\n",
    "            '''\n",
    "            convert object to attributes to save\n",
    "            '''\n",
    "            serialized_node_list = []\n",
    "            for node in node_list:\n",
    "                # follow the same order of attributes in partition class\n",
    "                attributes = [node.num_dims]\n",
    "                attributes += node.boundary\n",
    "                attributes.append(node.nid) # node id = its ow id\n",
    "                attributes.append(node.pid) # parent id\n",
    "                attributes.append(1 if node.is_irregular_shape_parent else 0)\n",
    "                attributes.append(1 if node.is_irregular_shape else 0)\n",
    "                attributes.append(node.num_children) # number of children\n",
    "                attributes += node.children_ids\n",
    "                attributes.append(1 if node.is_leaf else 0)\n",
    "                attributes.append(node.node_size)\n",
    "                \n",
    "                serialized_node_list.append(attributes)\n",
    "            return serialized_node_list\n",
    "        \n",
    "        def __build_tree_from_serialized_node_list(self, serialized_node_list):\n",
    "            \n",
    "            self.pt_root = None\n",
    "            self.nid_node_dict.clear()\n",
    "            \n",
    "            for serialized_node in serialized_node_list:\n",
    "                num_dims = serialized_node[0]\n",
    "                boundary = serialized_node[1: 1+2*num_dims]\n",
    "                nid = serialized_node[1+2*num_dims] # node id\n",
    "                pid = serialized_node[2+2*num_dims] # parent id\n",
    "                is_irregular_shape_parent = False if serialized_node[3+2*num_dims] == 0 else True\n",
    "                is_irregular_shape = False if serialized_node[4+2*num_dims] == 0 else True\n",
    "                num_children = serialized_node[5+2*num_dims]\n",
    "                children_ids = []\n",
    "                if num_children != 0:\n",
    "                    children_ids = serialized_node[1+5+2*num_dims: 1+num_children+1+5+2*num_dims] # +1 for the end exclusive\n",
    "                is_leaf = False if serialized_node[1+num_children+5+2*num_dims] == 0 else True\n",
    "                node_size = serialized_node[2+num_children+5+2*num_dims] # don't use -1 in case of match error\n",
    "                \n",
    "                node = PartitionNode(num_dims, boundary, nid, pid, is_irregular_shape_parent, \n",
    "                                     is_irregular_shape, num_children, children_ids, is_leaf, node_size)\n",
    "                self.nid_node_dict[nid] = node # update dict\n",
    "                \n",
    "            self.pt_root = self.nid_node_dict[0]\n",
    "            \n",
    "        def __find_overlapped_partition(self, node, query):\n",
    "            \n",
    "            if node.is_leaf:\n",
    "                return [node.nid] if node.is_overlap(query) > 0 else []\n",
    "            \n",
    "            node_id_list = []\n",
    "            if node.is_overlap(query) <= 0:\n",
    "                pass\n",
    "            elif node.is_irregular_shape_parent: # special process for irregular shape partitions!\n",
    "                overlap_irregular_shape_node_flag = False\n",
    "                for nid in node.children_ids[0: -1]: # except the last one, should be the irregular shape partition\n",
    "                    overlap_case = self.nid_node_dict[nid].is_overlap(query)\n",
    "                    if overlap_case == 1:\n",
    "                        overlap_irregular_shape_node_flag = True\n",
    "                    if overlap_case > 0:\n",
    "                        node_id_list.append(nid)      \n",
    "                if overlap_irregular_shape_node_flag:\n",
    "                    node_id_list.append(node.children_ids[-1])\n",
    "            else:  \n",
    "                for nid in node.children_ids:\n",
    "                    print(\"nid: \", nid)\n",
    "                    node_id_list += self.__find_overlapped_partition(self.nid_node_dict[nid], query)\n",
    "            return node_id_list\n",
    "        \n",
    "        def __visualize_2d(self, dims, queries = [], path = None):\n",
    "            fig, ax = plt.subplots(1)\n",
    "            \n",
    "            num_dims = self.pt_root.num_dims\n",
    "            plt.xlim(self.pt_root.boundary[dims[0]], self.pt_root.boundary[dims[0]+num_dims])\n",
    "            plt.ylim(self.pt_root.boundary[dims[1]], self.pt_root.boundary[dims[1]+num_dims])\n",
    "            \n",
    "            leaves = self.get_leaves()\n",
    "            for leaf in leaves:\n",
    "                \n",
    "                lower1 = leaf.boundary[dims[0]]\n",
    "                lower2 = leaf.boundary[dims[1]]             \n",
    "                upper1 = leaf.boundary[dims[0]+num_dims]\n",
    "                upper2 = leaf.boundary[dims[1]+num_dims]\n",
    "                \n",
    "                rect = Rectangle((lower1,lower2),upper1-lower1,upper2-lower2,fill=False,edgecolor='g',linewidth=1)\n",
    "                ax.add_patch(rect)\n",
    "                   \n",
    "            for query in queries:\n",
    "\n",
    "                lower1 = query[dims[0]]\n",
    "                lower2 = query[dims[1]]  \n",
    "                upper1 = query[dims[0]+num_dims]\n",
    "                upper2 = query[dims[1]+num_dims]\n",
    "\n",
    "                rect = Rectangle((lower1,lower2),upper1-lower1,upper2-lower2,fill=False,edgecolor='r',linewidth=1)\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            ax.set_xlabel('dim 1', fontsize=15)\n",
    "            ax.set_ylabel('dim 2', fontsize=15)\n",
    "            #plt.xticks(np.arange(0, 400001, 100000), fontsize=10)\n",
    "            #plt.yticks(np.arange(0, 20001, 5000), fontsize=10)\n",
    "\n",
    "            plt.tight_layout() # preventing clipping the labels when save to pdf\n",
    "\n",
    "            if path != None:\n",
    "                fig.savefig(path)\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "        %matplotlib notebook\n",
    "        def __visualize_3d(self, dims, queries = [], path = None):\n",
    "            fig = plt.figure()\n",
    "            ax = Axes3D(fig)\n",
    "            \n",
    "            num_dims = self.pt_root.num_dims\n",
    "            plt.xlim(self.pt_root.boundary[dims[0]], self.pt_root.boundary[dims[0]+num_dims])\n",
    "            plt.ylim(self.pt_root.boundary[dims[1]], self.pt_root.boundary[dims[1]+num_dims])\n",
    "            ax.set_zlim(self.pt_root.boundary[dims[2]], self.pt_root.boundary[dims[2]+num_dims])\n",
    "            \n",
    "            leaves = self.get_leaves()\n",
    "            for leaf in leaves:\n",
    "                \n",
    "                L1 = leaf.boundary[dims[0]]\n",
    "                L2 = leaf.boundary[dims[1]]\n",
    "                L3 = leaf.boundary[dims[2]]      \n",
    "                U1 = leaf.boundary[dims[0]+num_dims]\n",
    "                U2 = leaf.boundary[dims[1]+num_dims]\n",
    "                U3 = leaf.boundary[dims[2]+num_dims]\n",
    "                \n",
    "                # the 12 lines to form a rectangle\n",
    "                x = [L1, U1]\n",
    "                y = [L2, L2]\n",
    "                z = [L3, L3]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                y = [U2, U2]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                z = [U3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                y = [L2, L2]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "\n",
    "                x = [L1, L1]\n",
    "                y = [L2, U2]\n",
    "                z = [L3, L3]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                x = [U1, U1]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                z = [U3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                x = [L1, L1]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "\n",
    "                x = [L1, L1]\n",
    "                y = [L2, L2]\n",
    "                z = [L3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                x = [U1, U1]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                y = [U2, U2]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "                x = [L1, L1]\n",
    "                ax.plot3D(x,y,z,color=\"g\")\n",
    "            \n",
    "            for query in queries:\n",
    "\n",
    "                L1 = query[dims[0]]\n",
    "                L2 = query[dims[1]]\n",
    "                L3 = query[dims[2]]\n",
    "                U1 = query[dims[0]+num_dims]\n",
    "                U2 = query[dims[1]+num_dims]\n",
    "                U3 = query[dims[2]+num_dims]\n",
    "\n",
    "                # the 12 lines to form a rectangle\n",
    "                x = [L1, U1]\n",
    "                y = [L2, L2]\n",
    "                z = [L3, L3]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                y = [U2, U2]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                z = [U3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                y = [L2, L2]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "\n",
    "                x = [L1, L1]\n",
    "                y = [L2, U2]\n",
    "                z = [L3, L3]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                x = [U1, U1]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                z = [U3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                x = [L1, L1]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "\n",
    "                x = [L1, L1]\n",
    "                y = [L2, L2]\n",
    "                z = [L3, U3]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                x = [U1, U1]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                y = [U2, U2]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "                x = [L1, L1]\n",
    "                ax.plot3D(x,y,z,color=\"r\")\n",
    "\n",
    "            if path != None:\n",
    "                fig.savefig(path)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitionAlgorithm:\n",
    "    '''\n",
    "    The partition algorithms, inlcuding NORA, QdTree and kd-tree.\n",
    "    '''\n",
    "    def __init__(self, data_threshold = 10000):\n",
    "        self.partition_tree = None\n",
    "        self.data_threshold = data_threshold\n",
    "    \n",
    "    \n",
    "    # = = = = = public functions (API) = = = = =\n",
    "    \n",
    "    def InitializeWithNORA(self, queries, num_dims, boundary, dataset, data_threshold):\n",
    "        self.partition_tree = PartitionTree(num_dims, boundary)\n",
    "        self.partition_tree.pt_root.node_size = len(dataset)\n",
    "        self.partition_tree.pt_root.dataset = dataset\n",
    "        self.partition_tree.pt_root.queryset = queries # assume all queries overlap with the boundary\n",
    "        start_time = time.time()\n",
    "        self.__QDT(data_threshold)\n",
    "        end_time = time.time()\n",
    "        print(\"Build Time (s):\", end_time-start_time)\n",
    "    \n",
    "    def InitializeWithQDT(self, queries, num_dims, boundary, dataset, data_threshold):\n",
    "        '''\n",
    "        # should I also store the candidate cut positions in Partition Node ?\n",
    "        The dimension of queries should match the dimension of boundary and dataset!\n",
    "        '''\n",
    "        self.partition_tree = PartitionTree(num_dims, boundary)\n",
    "        self.partition_tree.pt_root.node_size = len(dataset)\n",
    "        self.partition_tree.pt_root.dataset = dataset\n",
    "        self.partition_tree.pt_root.queryset = queries # assume all queries overlap with the boundary\n",
    "        start_time = time.time()\n",
    "        self.__QDT(data_threshold)\n",
    "        end_time = time.time()\n",
    "        print(\"Build Time (s):\", end_time-start_time)\n",
    "     \n",
    "    def InitializeWithKDT(self, num_dims, boundary, dataset, data_threshold):\n",
    "        '''\n",
    "        num_dims denotes the (first) number of dimension to split, usually it should correspond with the boundary\n",
    "        rewrite the KDT using PartitionTree data structure\n",
    "        call the recursive __KDT methods\n",
    "        '''\n",
    "        self.partition_tree = PartitionTree(num_dims, boundary)\n",
    "        self.partition_tree.pt_root.node_size = len(dataset)\n",
    "        self.partition_tree.pt_root.dataset = dataset\n",
    "        # start from the first dimension\n",
    "        start_time = time.time()\n",
    "        self.__KDT(0, data_threshold, self.partition_tree.pt_root)\n",
    "        end_time = time.time()\n",
    "        print(\"Build Time (s):\", end_time-start_time)\n",
    "    \n",
    "    def ContinuePartitionWithKDT(self, existing_partition_tree, data_threshold):\n",
    "        '''\n",
    "        pass in a PartitionTree instance\n",
    "        then keep partition its leaf nodes with KDT, if available\n",
    "        '''\n",
    "        self.partition_tree = existing_partition_tree\n",
    "        leaves = existing_partition_tree.get_leaves()\n",
    "        for leaf in leaves:\n",
    "            self.__KDT(0, data_threshold, leaf)\n",
    "    \n",
    "    \n",
    "    # = = = = = internal functions = = = = =\n",
    "    \n",
    "    def __max_bound(self, num_dims, queryset):\n",
    "        '''\n",
    "        bound the queries by their maximum bounding rectangle\n",
    "        '''\n",
    "        max_bound_L = np.amin(np.array(queryset)[:,0:num_dims],axis=0).tolist()\n",
    "        max_bound_U = np.amax(np.array(queryset)[:,num_dims:],axis=0).tolist()\n",
    "        max_bound = max_bound_L + max_bound_U # concat\n",
    "        return max_bound\n",
    "    \n",
    "    def __NORA(self, data_threshold):\n",
    "        \n",
    "        CanSplit = True\n",
    "        while CanSplit:\n",
    "            CanSplit = False           \n",
    "            \n",
    "            # for leaf in self.partition_tree.get_leaves():\n",
    "            leaves = self.partition_tree.get_leaves()\n",
    "            print(\"# number of leaf nodes:\",len(leaves))\n",
    "            for leaf in leaves:\n",
    "                \n",
    "                print(\"current leaf node id:\",leaf.nid, \"leaf node dataset size:\",len(leaf.dataset))\n",
    "                if leaf.node_size < 2 * data_threshold:\n",
    "                    continue\n",
    "                    \n",
    "                # get best candidate cut position\n",
    "                skip, max_skip, max_skip_split_dim, max_skip_split_value, max_skip_split_type = 0, -1, 0, 0, 0\n",
    "                # extend the candidate cut with medians when it reach the bottom\n",
    "                candidate_cuts = leaf.get_candidate_cuts(True) if leaf.node_size < 4 * data_threshold else leaf.get_candidate_cuts()     \n",
    "                             \n",
    "                for split_dim, split_value in candidate_cuts:\n",
    "\n",
    "                    # first try normal split\n",
    "                    skip, left_size, right_size = leaf.if_split(split_dim, split_value, data_threshold)\n",
    "                    if skip > max_skip:\n",
    "                        max_skip, max_skip_split_dim, max_skip_split_value, max_skip_split_type = skip, split_dim, split_value, 0\n",
    "\n",
    "                    # if it's available for bounding split, try it\n",
    "                    if leaf.node_size < 3 * data_threshold:\n",
    "                        # try bounding split\n",
    "                        valid, skip = leaf.if_bounding_split(data_threshold, approximate = False)\n",
    "                        if valid and skip > max_skip:\n",
    "                            max_skip, max_skip_split_dim, max_skip_split_value, max_skip_split_type = skip, split_dim, split_value, 1\n",
    "\n",
    "                    # if it's availble for dual-bounding split, try it\n",
    "                    elif leaf.node_size < 4 * data_threshold and left_size < 2 * data_threshold and right_size < 2 * data_threshold:\n",
    "                        # try dual-bounding split              \n",
    "                        valid, skip = leaf.if_dual_bounding_split(split_dim, split_value, data_threshold, approximate = False)\n",
    "                        if valid and skip > max_skip:\n",
    "                            max_skip, max_skip_split_dim, max_skip_split_value, max_skip_split_type = skip, split_dim, split_value, 2\n",
    "\n",
    "                if max_skip > 0:\n",
    "                    # if the cost become smaller, apply the cut\n",
    "                    child_node1, child_node2 = self.partition_tree.apply_split(leaf.nid, max_skip_split_dim, max_skip_split_value, max_skip_split_type)\n",
    "                    print(\" Split on node id:\", leaf.nid)\n",
    "                    CanSplit = True\n",
    "                    \n",
    "    \n",
    "    def __QDT(self, data_threshold):\n",
    "        \n",
    "        CanSplit = True\n",
    "        while CanSplit:\n",
    "            CanSplit = False           \n",
    "            \n",
    "            # for leaf in self.partition_tree.get_leaves():\n",
    "            leaves = self.partition_tree.get_leaves()\n",
    "            print(\"# number of leaf nodes:\",len(leaves))\n",
    "            for leaf in leaves:\n",
    "                     \n",
    "                # print(\"current leaf node id:\",leaf.nid, \"leaf node dataset size:\",len(leaf.dataset))\n",
    "                if leaf.node_size < 2 * data_threshold:\n",
    "                    continue\n",
    "                \n",
    "                candidate_cuts = leaf.get_candidate_cuts()\n",
    "                \n",
    "                # get best candidate cut position\n",
    "                skip, max_skip, max_skip_split_dim, max_skip_split_value = 0, -1, 0, 0\n",
    "                for split_dim, split_value in candidate_cuts:\n",
    "\n",
    "                    skip,_,_ = leaf.if_split(split_dim, split_value, data_threshold)\n",
    "                    if skip > max_skip:\n",
    "                        max_skip = skip\n",
    "                        max_skip_split_dim = split_dim\n",
    "                        max_skip_split_value = split_value\n",
    "\n",
    "                if max_skip > 0:\n",
    "                    # if the cost become smaller, apply the cut\n",
    "                    child_node1, child_node2 = self.partition_tree.apply_split(leaf.nid, max_skip_split_dim, max_skip_split_value)\n",
    "                    print(\" Split on node id:\", leaf.nid)\n",
    "                    CanSplit = True\n",
    "            \n",
    "    \n",
    "    def __KDT(self, current_dim, data_threshold, current_node):\n",
    "        '''\n",
    "        Store the dataset in PartitionNode: we can keep it, but only as a tempoary attribute\n",
    "        '''\n",
    "        # cannot be further split\n",
    "        if current_node.node_size < 2 * data_threshold:\n",
    "            return   \n",
    "        \n",
    "        # split the node into equal halves by its current split dimension\n",
    "        median = np.median(current_node.dataset[:,current_dim])\n",
    "        \n",
    "        sub_dataset1_size = np.count_nonzero(current_node.dataset[:,current_dim] < median)\n",
    "        sub_dataset2_size = len(current_node.dataset) - sub_dataset1_size\n",
    "        \n",
    "        if sub_dataset1_size < data_threshold or sub_dataset2_size < data_threshold:\n",
    "            pass\n",
    "        else:\n",
    "            child_node1, child_node2 = self.partition_tree.apply_split(current_node.nid, current_dim, median)\n",
    "            \n",
    "            # update next split dimension\n",
    "            current_dim += 1\n",
    "            if current_dim >= current_node.num_dims:\n",
    "                current_dim %= current_node.num_dims\n",
    "    \n",
    "            # recursive call on sub nodes\n",
    "            self.__KDT(current_dim, data_threshold, child_node1)\n",
    "            self.__KDT(current_dim, data_threshold, child_node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAndQuerysetHelper:\n",
    "    '''\n",
    "    naming:\n",
    "    dataset: [base_path]/dataset/lineitem_[scale_factor]_[prob_threshold].csv\n",
    "    domain: [base_path]/dataset/lineitem_[scale_factor]_[prob_threshold]_domains.csv\n",
    "    queryset: [base_path]/queryset/[prob]/[vary_item]/[vary_val]_[used_dimensions]_[distribution/random].csv\n",
    "    '''    \n",
    "    def __init__(self, used_dimensions = None, scale_factor = 100, base_path = 'C:/Users/Cloud/iCloudDrive/NORA_experiments',\n",
    "                prob_id = 1, vary_id = 0, vary_val = 0, train_percent = 0.5, random_percent = 0):\n",
    "        \n",
    "        self.used_dimensions = used_dimensions # i.e., [1,2,3,4]\n",
    "        self.total_dims = 16 # the dimensions of lineitem table\n",
    "        self.domain_dims = 8 # the dimensions we used for split and maintain min max for\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        self.prob_threshold = 1 / self.scale_factor # the probability of an original record being sampled into this dataset\n",
    "        self.block_size = 1000000 // self.scale_factor # in original file, 1M rows take approximately 128MB\n",
    "        \n",
    "        self.base_path = base_path\n",
    "        self.save_path_data = base_path + '/dataset/lineitem_' + str(scale_factor) + '_' + str(self.prob_threshold) + '.csv'\n",
    "        self.save_path_domain = base_path + '/dataset/lineitem_' + str(scale_factor) + '_' + str(self.prob_threshold) + '_domains.csv'\n",
    "        \n",
    "        self.vary_items = ['default', 'alpha', 'num_dims', 'prob_dims', 'num_X']\n",
    "        self.vary_id = vary_id\n",
    "        self.vary_val = vary_val\n",
    "        \n",
    "        self.prob_id = prob_id\n",
    "        self.query_base_path = self.base_path + '/queryset/prob' + str(self.prob_id) + '/' + self.vary_items[vary_id] + '/'\n",
    "        self.query_file_name = str(vary_val) + '_' + str(self.used_dimensions) # dependent on used_dimensions, so change dim first\n",
    "        \n",
    "        self.query_distribution_path = self.query_base_path + self.query_file_name + '_distribution.csv'\n",
    "        self.query_random_path = self.query_base_path + self.query_file_name + '_random.csv'\n",
    "        \n",
    "        self.train_percent = train_percent\n",
    "        \n",
    "        # the following are default query generation settings\n",
    "        self.random_percent = random_percent # usef for query generation\n",
    "        self.cluster_center_amount = 10\n",
    "        self.maximum_range_percent = 0.1 # 10% of the corresponding domain\n",
    "        self.sigma_percent = 0.2 # control the differences in a cluster\n",
    "        \n",
    "    \n",
    "    # = = = = = public functions (API) = = = = =\n",
    "    \n",
    "    def set_config(self, scale_factor = 100, base_path = 'C:/Users/Cloud/iCloudDrive/NORA_experiments', \n",
    "                   used_dimensions = None, prob_id = 1, vary_id = 0, vary_val = 0):\n",
    "        '''\n",
    "        As many attributes are related to each other, this is used to refresh the whole settings.\n",
    "        '''\n",
    "        self.used_dimensions = used_dimensions\n",
    "        self.scale_factor = scale_factor\n",
    "        self.prob_threshold = 1 / self.scale_factor\n",
    "        self.block_size = 1000000 // self.scale_factor # in original file, 1M rows take approximately 128MB\n",
    "        self.base_path = base_path\n",
    "        self.save_path_data = base_path + '/dataset/lineitem_' + str(scale_factor) + '_' + str(self.prob_threshold) + '.csv'\n",
    "        self.save_path_domain = base_path + '/dataset/lineitem_' + str(scale_factor) + '_' + str(self.prob_threshold) + '_domains.csv'\n",
    "        self.vary_id = vary_id\n",
    "        self.vary_val = vary_val\n",
    "        self.prob_id = prob_id\n",
    "        self.query_base_path = self.base_path + '/queryset/prob' + str(self.prob_id) + '/' + self.vary_items[vary_id] + '/'\n",
    "        self.query_file_name = str(vary_val) + '_' + str(self.used_dimensions)\n",
    "        self.query_distribution_path = self.query_base_path + self.query_file_name + '_distribution.csv'\n",
    "        self.query_random_path = self.query_base_path + self.query_file_name + '_random.csv'    \n",
    "    \n",
    "    def load_dataset(self, used_dimensions = []):\n",
    "        '''\n",
    "        the priority of the used_dimensions argument in the function is higher than the saved attribute version\n",
    "        domains: [[L1, U1], [L2, U2],...]\n",
    "        return the dataset projected on selected dimensions\n",
    "        '''\n",
    "        dataset = np.genfromtxt(save_path_data, delimiter=',') # the sampled subset\n",
    "        domains = np.genfromtxt(save_path_domain, delimiter=',') # the domain of that scale\n",
    "        if used_dimensions != []:\n",
    "            dataset = dataset[:,used_dimensions]\n",
    "            domains = domains[used_dimensions]\n",
    "        elif self.used_dimensions is not None:\n",
    "            dataset = dataset[:,self.used_dimensions]\n",
    "            domains = domains[self.used_dimensions]\n",
    "        return dataset, domains\n",
    "    \n",
    "    def load_queryset(self, return_train_test = True, query_distribution_path = None, query_random_path = None):\n",
    "        '''\n",
    "        query is in plain form, i.e., [l1,l2,...,ln, u1,u2,...,un]\n",
    "        how about the used dimension?\n",
    "        return the saved queryset, should be projected on selected dimensions.\n",
    "        '''\n",
    "        # embed used_dimension info into query file's name\n",
    "        # when load, will auto matically check whether used_dimension is matched!!! or load will failed\n",
    "        \n",
    "        distribution_query, random_query = None, None\n",
    "        \n",
    "        if query_distribution_path is not None and query_random_path is not None:\n",
    "            distribution_query = np.genfromtxt(query_distribution_path, delimiter=',')\n",
    "            random_query = np.genfromtxt(query_random_path, delimiter=',')\n",
    "        else:\n",
    "            distribution_query = np.genfromtxt(self.query_distribution_path, delimiter=',')\n",
    "            random_query = np.genfromtxt(self.query_random_path, delimiter=',')\n",
    "        \n",
    "        if return_train_test:\n",
    "            training_set, testing_set = self.__convert_to_train_test(distribution_query, random_query)\n",
    "            return training_set, testing_set\n",
    "        else:\n",
    "            return distribution_query, random_query\n",
    "    \n",
    "    def generate_dataset_and_save(self, original_table_path, chunk_size = 100000):\n",
    "        '''\n",
    "        refer to TPCH tools to generate the original dataset (.tbl)\n",
    "        this function is used to process the .tbl file with given sampling rate to generate a .csv file\n",
    "        consider the possible table size, this function is implemented in a batch processing manner\n",
    "        '''\n",
    "        sampled_subset = []\n",
    "        domains = [[float('Infinity'), float('-Infinity')] for i in range(self.domain_dims)] # indicate min, max\n",
    "        \n",
    "        col_names = ['_c'+str(i) for i in range(self.total_dims)]\n",
    "        cols = [i for i in range(self.total_dims)]\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        batch_count = 0\n",
    "        for chunk in pd.read_table(original_table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "            print('current chunk: ', batch_count)\n",
    "            chunk.apply(lambda row: self.__process_chunk_sampling(row, domains, sampled_subset), axis=1)\n",
    "            batch_count += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('total processing time: ', end_time - start_time)\n",
    "        \n",
    "        sampled_subset = np.array(sampled_subset)\n",
    "        domains = np.array(domains)\n",
    "        np.savetxt(self.save_path_data, sampled_subset, delimiter=',')\n",
    "        np.savetxt(self.save_path_domain, domains, delimiter=',')\n",
    "    \n",
    "    def generate_queryset_and_save(self, query_amount, dim_prob = [], prob_id = 1, vary_id = 0, vary_val = 0, return_train_test = True):\n",
    "        '''\n",
    "        generate queryset for given dimensions.\n",
    "        query_amount: total query amount, including distribution queries and random queries\n",
    "        dim_prob: the probability of using a given dimension (in used_dimensions) in a query\n",
    "        other configurations are stored in class attributes\n",
    "        REMEMBER to change the used_dimensions first if not using the previous one !!!\n",
    "        \n",
    "        the returned queries are not numpy object by default\n",
    "        '''\n",
    "        num_random_query = int(query_amount * self.random_percent)\n",
    "        num_distribution_query = query_amount - num_random_query\n",
    "        domains = np.genfromtxt(save_path_domain, delimiter=',')[self.used_dimensions]\n",
    "        if dim_prob == []: # by default, use all the selected dimensions\n",
    "            dim_prob = [1 for i in range(len(self.used_dimensions))]  \n",
    "        maximum_range = [(domains[i,1] - domains[i,0]) * self.maximum_range_percent for i in range(len(domains))]\n",
    "        \n",
    "        distribution_query = self.__generate_distribution_query(num_distribution_query, dim_prob, domains, maximum_range)\n",
    "        random_query = self.__generate_random_query(num_random_query, dim_prob, domains, maximum_range)\n",
    "        \n",
    "        # refresh query related class attributes\n",
    "        self.vary_id = vary_id\n",
    "        self.vary_val = vary_val      \n",
    "        self.query_base_path = self.base_path + '/queryset/prob' + str(prob_id) + '/' + self.vary_items[vary_id] + '/'\n",
    "        self.query_file_name = str(vary_val) + '_' + str(self.used_dimensions)\n",
    "        self.query_distribution_path = self.query_base_path + self.query_file_name + '_distribution.csv'\n",
    "        self.query_random_path = self.query_base_path + self.query_file_name + '_random.csv'\n",
    "        \n",
    "        # save\n",
    "        np.savetxt(self.save_path_data, distribution_query, delimiter=',')\n",
    "        np.savetxt(self.save_path_domain, random_query, delimiter=',')\n",
    "        \n",
    "        # print(\" = = = distribution query = = = \")\n",
    "        # print(distribution_query)\n",
    "        \n",
    "        # print(\" = = = random query = = = \")\n",
    "        # print(random_query)\n",
    "        \n",
    "        if return_train_test:\n",
    "            training_set, testing_set = self.__convert_to_train_test(distribution_query, random_query)\n",
    "            return training_set, testing_set\n",
    "        else:\n",
    "            return distribution_query, random_query\n",
    "        \n",
    "    \n",
    "    # = = = = = internal functions = = = = =\n",
    "    \n",
    "    def __process_chunk_sampling(self, row, domains, sampled_subset):\n",
    "        prob = random.uniform(0, 1)\n",
    "        row_numpy = row.to_numpy()  \n",
    "        for i in range(len(domains)):\n",
    "            if row_numpy[i] > domains[i][1]:\n",
    "                domains[i][1] = row_numpy[i]\n",
    "            if row_numpy[i] < domains[i][0]:\n",
    "                domains[i][0] = row_numpy[i]\n",
    "        if prob <= self.prob_threshold:    \n",
    "            sampled_subset.append(row_numpy[0:self.domain_dims].tolist())\n",
    "    \n",
    "    def __convert_to_train_test(self, distribution_query, random_query):\n",
    "        train_distribution = distribution_query[0:int(self.train_percent*len(distribution_query))]\n",
    "        test_distribution = distribution_query[int(self.train_percent*len(distribution_query)):]\n",
    "        train_random = random_query[0:int(self.train_percent*len(random_query))]\n",
    "        test_random = random_query[int(self.train_percent*len(random_query)):]\n",
    "        \n",
    "        # to deal with the shape issue, 0 items cannot be concated\n",
    "        if len(distribution_query) == 0 and len(random_query) == 0:\n",
    "            return [], []\n",
    "        elif len(distribution_query) == 0:\n",
    "            return train_random, test_random\n",
    "        elif len(random_query) == 0:\n",
    "            return train_distribution, test_distribution\n",
    "        else:\n",
    "            training_set = np.concatenate((train_distribution, train_random), axis=0)\n",
    "            testing_set = np.concatenate((test_distribution, test_random), axis=0)\n",
    "            return training_set, testing_set\n",
    "    \n",
    "    def __generate_distribution_query(self, query_amount, dim_prob, domains, maximum_range):\n",
    "        \n",
    "        # first, generate cluster centers\n",
    "        centers = []\n",
    "        for i in range(self.cluster_center_amount):\n",
    "            center = [] # [D1, D2,..., Dk]\n",
    "            for k in range(len(domains)):\n",
    "                ck = random.uniform(domains[k][0], domains[k][1])\n",
    "                center.append(ck)\n",
    "            centers.append(center)\n",
    "\n",
    "        # second, generate expected range for each dimension for each center\n",
    "        centers_ranges = []\n",
    "        for i in range(self.cluster_center_amount):\n",
    "            ranges = [] # the range in all dimensions for a given center\n",
    "            for k in range(len(domains)):\n",
    "                ran = random.uniform(0, maximum_range[k])\n",
    "                ranges.append(ran)\n",
    "            centers_ranges.append(ranges)\n",
    "\n",
    "        # third, generate sigma for each dimension for each center\n",
    "        centers_sigmas = []\n",
    "        for i in range(self.cluster_center_amount):\n",
    "            sigmas = []\n",
    "            for k in range(len(domains)):\n",
    "                sigma = random.uniform(0, maximum_range[k] * self.sigma_percent)\n",
    "                sigmas.append(sigma)\n",
    "            centers_sigmas.append(sigmas)\n",
    "\n",
    "        # fourth, generate queries\n",
    "        distribution_query = [] = []\n",
    "        for i in range(query_amount):\n",
    "            # choose a center\n",
    "            center_index = random.randint(0, self.cluster_center_amount-1) # this is inclusive            \n",
    "            query_lower, query_upper = [], []\n",
    "            for k in range(len(domains)):\n",
    "                # consider whether or not to use this dimension\n",
    "                L, U = None, None\n",
    "                prob = random.uniform(0, 1)\n",
    "                if prob > dim_prob[k]:\n",
    "                    L = domains[k][0]\n",
    "                    U = domains[k][1]\n",
    "                else:\n",
    "                    center = centers[center_index]\n",
    "                    query_range = centers_ranges[center_index][k]\n",
    "                    L = center[k] - query_range/2\n",
    "                    U = center[k] + query_range/2\n",
    "                    L = random.gauss(L, centers_sigmas[center_index][k])\n",
    "                    U = random.gauss(U, centers_sigmas[center_index][k])\n",
    "                    if L <= domains[k][0]:\n",
    "                        L = domains[k][0]\n",
    "                    if U >= domains[k][1]:\n",
    "                        U = domains[k][1]\n",
    "                    if L > U:\n",
    "                        L, U = U, L\n",
    "                query_lower.append(L)\n",
    "                query_upper.append(U)\n",
    "            distribution_query.append(query_lower + query_upper)\n",
    "        return distribution_query\n",
    "    \n",
    "    def __generate_random_query(self, query_amount, dim_prob, domains, maximum_range):\n",
    "        random_query = []\n",
    "        for i in range(query_amount):\n",
    "            query_lower, query_upper = [], []\n",
    "            for k in range(len(domains)):     \n",
    "                # consider whether or not to use this dimension\n",
    "                L, U = None, None\n",
    "                prob = random.uniform(0, 1)\n",
    "                if prob > dim_prob[k]:\n",
    "                    L = domains[k][0]\n",
    "                    U = domains[k][1]\n",
    "                else:\n",
    "                    center = random.uniform(domains[k][0], domains[k][1])\n",
    "                    query_range = random.uniform(0, self.maximum_range[k])\n",
    "                    L = center - query_range/2\n",
    "                    U = center + query_range/2\n",
    "                    if L <= domainS[k][0]:\n",
    "                        L = domain[k][0]\n",
    "                    if U >= domain[k][1]:\n",
    "                        U = domain[k][1]\n",
    "                query_lower.append(L)\n",
    "                query_upper.append(U)\n",
    "            random_query.append(query_lower + query_upper)\n",
    "        return random_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_node = PartitionNode(num_dims = 4, boundary = [0,0,1,2,3,3,3,3], nid = 1, pid = 0, is_irregular_shape_parent = False,\n",
    "#                  is_irregular_shape = False, num_children = 0, children_ids = [], is_leaf = True, node_size = 20)\n",
    "\n",
    "# pa.partition_tree.add_node(87, new_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Test Dataset and Queryset Loader = = = \n",
    "helper = DatasetAndQuerysetHelper(used_dimensions = [1,2,3,4])\n",
    "training_set, testing_set = helper.generate_queryset_and_save(100)\n",
    "# training_set, testing_set = helper.load_queryset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_set.shape\n",
    "# testing_set\n",
    "dataset, domains = helper.load_dataset([1,2,3,4])\n",
    "# len(dataset) # 6001309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Time (s): 3.542032241821289\n"
     ]
    }
   ],
   "source": [
    "# = = = = = Test PartitionAlgorithm (KDT) = = = = = \n",
    "pa1 = PartitionAlgorithm()\n",
    "boundary = [interval[0] for interval in domains]+[interval[1] for interval in domains]\n",
    "pa1.InitializeWithKDT(4, boundary, dataset, data_threshold = 10000) # 447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  24\n",
      "nid:  47\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  53\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  54\n",
      "nid:  48\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  182\n",
      "nid:  193\n",
      "nid:  194\n",
      "nid:  197\n",
      "nid:  199\n",
      "nid:  200\n",
      "nid:  198\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  182\n",
      "nid:  193\n",
      "nid:  194\n",
      "nid:  197\n",
      "nid:  199\n",
      "nid:  200\n",
      "nid:  198\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  137\n",
      "nid:  139\n",
      "nid:  140\n",
      "nid:  138\n",
      "nid:  136\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  183\n",
      "nid:  185\n",
      "nid:  186\n",
      "nid:  184\n",
      "nid:  182\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  137\n",
      "nid:  139\n",
      "nid:  140\n",
      "nid:  138\n",
      "nid:  136\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  183\n",
      "nid:  185\n",
      "nid:  186\n",
      "nid:  184\n",
      "nid:  182\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  357\n",
      "nid:  359\n",
      "nid:  360\n",
      "nid:  363\n",
      "nid:  365\n",
      "nid:  366\n",
      "nid:  364\n",
      "nid:  358\n",
      "nid:  356\n",
      "nid:  354\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  182\n",
      "nid:  193\n",
      "nid:  194\n",
      "nid:  197\n",
      "nid:  199\n",
      "nid:  200\n",
      "nid:  198\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  137\n",
      "nid:  139\n",
      "nid:  140\n",
      "nid:  138\n",
      "nid:  136\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  183\n",
      "nid:  185\n",
      "nid:  186\n",
      "nid:  184\n",
      "nid:  182\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  137\n",
      "nid:  138\n",
      "nid:  141\n",
      "nid:  142\n",
      "nid:  145\n",
      "nid:  146\n",
      "nid:  136\n",
      "nid:  134\n",
      "nid:  157\n",
      "nid:  159\n",
      "nid:  160\n",
      "nid:  163\n",
      "nid:  164\n",
      "nid:  167\n",
      "nid:  168\n",
      "nid:  158\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  183\n",
      "nid:  184\n",
      "nid:  187\n",
      "nid:  189\n",
      "nid:  190\n",
      "nid:  188\n",
      "nid:  182\n",
      "nid:  180\n",
      "nid:  203\n",
      "nid:  205\n",
      "nid:  206\n",
      "nid:  209\n",
      "nid:  211\n",
      "nid:  212\n",
      "nid:  210\n",
      "nid:  204\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  8\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  182\n",
      "nid:  193\n",
      "nid:  194\n",
      "nid:  197\n",
      "nid:  199\n",
      "nid:  200\n",
      "nid:  198\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  24\n",
      "nid:  47\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  53\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  54\n",
      "nid:  48\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  24\n",
      "nid:  47\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  53\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  54\n",
      "nid:  48\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  6\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  137\n",
      "nid:  138\n",
      "nid:  141\n",
      "nid:  142\n",
      "nid:  145\n",
      "nid:  146\n",
      "nid:  136\n",
      "nid:  134\n",
      "nid:  157\n",
      "nid:  159\n",
      "nid:  160\n",
      "nid:  163\n",
      "nid:  164\n",
      "nid:  167\n",
      "nid:  168\n",
      "nid:  158\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  183\n",
      "nid:  184\n",
      "nid:  187\n",
      "nid:  189\n",
      "nid:  190\n",
      "nid:  188\n",
      "nid:  182\n",
      "nid:  180\n",
      "nid:  203\n",
      "nid:  205\n",
      "nid:  206\n",
      "nid:  209\n",
      "nid:  211\n",
      "nid:  212\n",
      "nid:  210\n",
      "nid:  204\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  179\n",
      "nid:  181\n",
      "nid:  182\n",
      "nid:  193\n",
      "nid:  194\n",
      "nid:  197\n",
      "nid:  199\n",
      "nid:  200\n",
      "nid:  198\n",
      "nid:  180\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  24\n",
      "nid:  47\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  53\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  54\n",
      "nid:  48\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  356\n",
      "nid:  379\n",
      "nid:  380\n",
      "nid:  391\n",
      "nid:  392\n",
      "nid:  395\n",
      "nid:  396\n",
      "nid:  399\n",
      "nid:  400\n",
      "nid:  354\n",
      "nid:  401\n",
      "nid:  402\n",
      "nid:  425\n",
      "nid:  426\n",
      "nid:  437\n",
      "nid:  438\n",
      "nid:  441\n",
      "nid:  443\n",
      "nid:  444\n",
      "nid:  442\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  116\n",
      "nid:  131\n",
      "nid:  133\n",
      "nid:  135\n",
      "nid:  136\n",
      "nid:  147\n",
      "nid:  148\n",
      "nid:  151\n",
      "nid:  152\n",
      "nid:  155\n",
      "nid:  156\n",
      "nid:  134\n",
      "nid:  132\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  21\n",
      "nid:  23\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  24\n",
      "nid:  22\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  225\n",
      "nid:  226\n",
      "nid:  337\n",
      "nid:  338\n",
      "nid:  353\n",
      "nid:  355\n",
      "nid:  357\n",
      "nid:  359\n",
      "nid:  360\n",
      "nid:  363\n",
      "nid:  365\n",
      "nid:  366\n",
      "nid:  364\n",
      "nid:  358\n",
      "nid:  356\n",
      "nid:  354\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  115\n",
      "nid:  117\n",
      "nid:  118\n",
      "nid:  125\n",
      "nid:  127\n",
      "nid:  128\n",
      "nid:  126\n",
      "nid:  116\n",
      "nid:  2\n",
      "Total logical IOs: 2896748\n",
      "Average logical IOs: 57934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2896748"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pa1.partition_tree.node_count\n",
    "# pa1.partition_tree.visualize(queries = training_set)\n",
    "pa1.partition_tree.evaluate_query_cost(training_set)\n",
    "\n",
    "# Total logical IOs: 2896748\n",
    "# Average logical IOs: 57934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pa1.partition_tree.nid_node_dict[0].children_ids\n",
    "# pa1.partition_tree.nid_node_dict[1].children_ids\n",
    "# pa1.partition_tree.nid_node_dict[2].children_ids\n",
    "# pa1.partition_tree.nid_node_dict[3].children_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of leaf nodes: 1\n",
      " Split on node id: 0\n",
      "# number of leaf nodes: 2\n",
      " Split on node id: 1\n",
      " Split on node id: 2\n",
      "# number of leaf nodes: 4\n",
      " Split on node id: 3\n",
      " Split on node id: 4\n",
      " Split on node id: 5\n",
      " Split on node id: 6\n",
      "# number of leaf nodes: 8\n",
      " Split on node id: 7\n",
      " Split on node id: 8\n",
      " Split on node id: 9\n",
      " Split on node id: 10\n",
      " Split on node id: 12\n",
      " Split on node id: 13\n",
      " Split on node id: 14\n",
      "# number of leaf nodes: 15\n",
      " Split on node id: 16\n",
      " Split on node id: 18\n",
      " Split on node id: 19\n",
      " Split on node id: 20\n",
      " Split on node id: 21\n",
      " Split on node id: 22\n",
      " Split on node id: 23\n",
      " Split on node id: 24\n",
      " Split on node id: 25\n",
      " Split on node id: 26\n",
      " Split on node id: 27\n",
      " Split on node id: 28\n",
      "# number of leaf nodes: 27\n",
      " Split on node id: 29\n",
      " Split on node id: 30\n",
      " Split on node id: 33\n",
      " Split on node id: 34\n",
      " Split on node id: 36\n",
      " Split on node id: 41\n",
      " Split on node id: 44\n",
      " Split on node id: 45\n",
      " Split on node id: 46\n",
      " Split on node id: 47\n",
      " Split on node id: 50\n",
      "# number of leaf nodes: 38\n",
      " Split on node id: 56\n",
      " Split on node id: 60\n",
      " Split on node id: 66\n",
      " Split on node id: 67\n",
      " Split on node id: 72\n",
      "# number of leaf nodes: 43\n",
      " Split on node id: 76\n",
      "# number of leaf nodes: 44\n",
      "Build Time (s): 8.3450026512146\n"
     ]
    }
   ],
   "source": [
    "# = = = = = Test PartitionAlgorithm (QDT) = = = = = \n",
    "pa2 = PartitionAlgorithm()\n",
    "boundary = [interval[0] for interval in domains]+[interval[1] for interval in domains]\n",
    "pa2.InitializeWithQDT(training_set, 4, boundary, dataset, data_threshold = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  17\n",
      "nid:  18\n",
      "nid:  31\n",
      "nid:  32\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  59\n",
      "nid:  60\n",
      "nid:  77\n",
      "nid:  78\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  61\n",
      "nid:  62\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  17\n",
      "nid:  18\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  39\n",
      "nid:  40\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  83\n",
      "nid:  84\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  73\n",
      "nid:  74\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  69\n",
      "nid:  70\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  73\n",
      "nid:  74\n",
      "nid:  28\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  75\n",
      "nid:  76\n",
      "nid:  85\n",
      "nid:  86\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  83\n",
      "nid:  84\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  69\n",
      "nid:  70\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  43\n",
      "nid:  44\n",
      "nid:  65\n",
      "nid:  66\n",
      "nid:  79\n",
      "nid:  80\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "Total logical IOs: 9824367\n",
      "Average logical IOs: 196487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9824367"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pa2.partition_tree.nid_node_dict[0].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[1].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[2].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[3].children_ids\n",
    "# pa2.partition_tree.node_count\n",
    "# pa2.partition_tree.visualize(queries = training_set)\n",
    "pa2.partition_tree.evaluate_query_cost(training_set) # Average logical IOs: 196487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa2.partition_tree.nid_node_dict[0].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[1].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[2].children_ids\n",
    "# pa2.partition_tree.nid_node_dict[3].children_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# number of leaf nodes: 1\n",
      " Split on node id: 0\n",
      "# number of leaf nodes: 2\n",
      " Split on node id: 1\n",
      " Split on node id: 2\n",
      "# number of leaf nodes: 4\n",
      " Split on node id: 3\n",
      " Split on node id: 4\n",
      " Split on node id: 5\n",
      " Split on node id: 6\n",
      "# number of leaf nodes: 8\n",
      " Split on node id: 7\n",
      " Split on node id: 8\n",
      " Split on node id: 9\n",
      " Split on node id: 10\n",
      " Split on node id: 12\n",
      " Split on node id: 13\n",
      " Split on node id: 14\n",
      "# number of leaf nodes: 15\n",
      " Split on node id: 16\n",
      " Split on node id: 18\n",
      " Split on node id: 19\n",
      " Split on node id: 20\n",
      " Split on node id: 21\n",
      " Split on node id: 22\n",
      " Split on node id: 23\n",
      " Split on node id: 24\n",
      " Split on node id: 25\n",
      " Split on node id: 26\n",
      " Split on node id: 27\n",
      " Split on node id: 28\n",
      "# number of leaf nodes: 27\n",
      " Split on node id: 29\n",
      " Split on node id: 30\n",
      " Split on node id: 33\n",
      " Split on node id: 34\n",
      " Split on node id: 36\n",
      " Split on node id: 41\n",
      " Split on node id: 44\n",
      " Split on node id: 45\n",
      " Split on node id: 46\n",
      " Split on node id: 47\n",
      " Split on node id: 50\n",
      "# number of leaf nodes: 38\n",
      " Split on node id: 56\n",
      " Split on node id: 60\n",
      " Split on node id: 66\n",
      " Split on node id: 67\n",
      " Split on node id: 72\n",
      "# number of leaf nodes: 43\n",
      " Split on node id: 76\n",
      "# number of leaf nodes: 44\n",
      "Build Time (s): 8.056007385253906\n"
     ]
    }
   ],
   "source": [
    "# = = = = = Test PartitionAlgorithm (NORA) = = = = = \n",
    "pa3 = PartitionAlgorithm()\n",
    "boundary = [interval[0] for interval in domains]+[interval[1] for interval in domains]\n",
    "pa3.InitializeWithNORA(training_set, 4, boundary, dataset, data_threshold = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  17\n",
      "nid:  18\n",
      "nid:  31\n",
      "nid:  32\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  59\n",
      "nid:  60\n",
      "nid:  77\n",
      "nid:  78\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  61\n",
      "nid:  62\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  8\n",
      "nid:  17\n",
      "nid:  18\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  39\n",
      "nid:  40\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  83\n",
      "nid:  84\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  33\n",
      "nid:  57\n",
      "nid:  58\n",
      "nid:  34\n",
      "nid:  20\n",
      "nid:  10\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  73\n",
      "nid:  74\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  69\n",
      "nid:  70\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  49\n",
      "nid:  50\n",
      "nid:  73\n",
      "nid:  74\n",
      "nid:  28\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  7\n",
      "nid:  15\n",
      "nid:  16\n",
      "nid:  29\n",
      "nid:  53\n",
      "nid:  54\n",
      "nid:  30\n",
      "nid:  55\n",
      "nid:  56\n",
      "nid:  75\n",
      "nid:  76\n",
      "nid:  85\n",
      "nid:  86\n",
      "nid:  8\n",
      "nid:  4\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  71\n",
      "nid:  72\n",
      "nid:  83\n",
      "nid:  84\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  19\n",
      "nid:  20\n",
      "nid:  35\n",
      "nid:  36\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  81\n",
      "nid:  82\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  14\n",
      "nid:  27\n",
      "nid:  28\n",
      "nid:  51\n",
      "nid:  52\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  46\n",
      "nid:  69\n",
      "nid:  70\n",
      "nid:  26\n",
      "nid:  47\n",
      "nid:  48\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  6\n",
      "nid:  13\n",
      "nid:  25\n",
      "nid:  45\n",
      "nid:  67\n",
      "nid:  68\n",
      "nid:  46\n",
      "nid:  26\n",
      "nid:  14\n",
      "nid:  1\n",
      "nid:  2\n",
      "nid:  5\n",
      "nid:  11\n",
      "nid:  12\n",
      "nid:  23\n",
      "nid:  41\n",
      "nid:  63\n",
      "nid:  64\n",
      "nid:  42\n",
      "nid:  24\n",
      "nid:  43\n",
      "nid:  44\n",
      "nid:  65\n",
      "nid:  66\n",
      "nid:  79\n",
      "nid:  80\n",
      "nid:  6\n",
      "nid:  1\n",
      "nid:  3\n",
      "nid:  4\n",
      "nid:  9\n",
      "nid:  10\n",
      "nid:  21\n",
      "nid:  37\n",
      "nid:  38\n",
      "nid:  22\n",
      "nid:  2\n",
      "Total logical IOs: 9824367\n",
      "Average logical IOs: 196487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9824367"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa3.partition_tree.evaluate_query_cost(training_set) # Average logical IOs: 196487, the same as QDT here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa3.partition_tree.nid_node_dict[0].children_ids # [1,2,1,2,1,2,1,2] WTF ???\n",
    "# pa3.partition_tree.nid_node_dict[1].children_ids\n",
    "# pa3.partition_tree.nid_node_dict[2].children_ids\n",
    "# pa3.partition_tree.nid_node_dict[3].children_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
