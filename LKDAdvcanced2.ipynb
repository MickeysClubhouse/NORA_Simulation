{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import import_ipynb # this can only be installed from pip (no conda)\n",
    "from QueryTree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Learned KD-Tree Split (Advanced 2 Version, consider query pruning) ===\n",
    "#\n",
    "# asssumption: the query boundings will not overlap. divide the KD-Tree recursively\n",
    "#\n",
    "# @dataset[i][k]: i: the ith record, k: the kth dimension  contains the data only in this subnode, ordered in original load order\n",
    "# @query: contains all the queries (bounded or original); ordered in original generated order;\n",
    "# @domains[k][0/1]:  k: dimension  0/1: min max   array object\n",
    "# @threshold: minimum partition size\n",
    "# @level: the current tree depth\n",
    "#\n",
    "# return @kdnodes: contains the domain of each node and the correpsonding records amount, notice the domain is\n",
    "# ordered by the original load order as dataset\n",
    "def LKDAdvanced2(dataset, query, domains, threshold, level, current_dim = 0):\n",
    "\n",
    "    # check if the threshold is already satisfied\n",
    "    total_size = len(dataset)\n",
    "    \n",
    "    #print(\"level: \",level, \"  size: \", total_size)\n",
    "    if total_size <= 2*threshold:\n",
    "        kdnodes = []\n",
    "        kdnodes.append([domains, total_size])\n",
    "        return kdnodes\n",
    "    \n",
    "    # create query tree\n",
    "    query_trees = []\n",
    "    for i in range(len(domains)):\n",
    "        qtree = QueryTree(i)\n",
    "        qtree.loadQuerySetFromQueries(query)\n",
    "        qtree.buildQueryTree()\n",
    "        query_trees.append(qtree)\n",
    "        \n",
    "    split_from_median_position_tag = False\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        split_from_median_position_tag = False\n",
    "        \n",
    "        split_distance_each_dim = []\n",
    "        split_value_each_dim = []\n",
    "        caches = []\n",
    "        medians = []\n",
    "\n",
    "        # for each dimension, we calculated the distance from median to its first non-cross split\n",
    "        for D in range(len(domains)):\n",
    "\n",
    "            # median, with fast median algorithm\n",
    "            median = np.median(dataset[:,D]) # the median value\n",
    "            split_value = median # by default (i.e., without shift), is median\n",
    "\n",
    "            # split distance\n",
    "            median_shift_distance_lower = 0\n",
    "            median_shift_distance_upper = 0\n",
    "            min_median_distance = 0\n",
    "            \n",
    "            if len(query_trees[D].node_dict) == 0:\n",
    "                split_distance_each_dim.append(min_median_distance)\n",
    "                split_value_each_dim.append(split_value)\n",
    "                continue\n",
    "            \n",
    "            # check if the default split position intersect some query\n",
    "            is_overlap, cache = query_trees[D].queryValue(median)\n",
    "\n",
    "            # if overlap, find out the shift distance\n",
    "            if is_overlap:\n",
    "                overlap_query_lower = cache[1]\n",
    "                overlap_query_upper = cache[2]\n",
    "\n",
    "                # check if the 2 ends exceeds the current domain\n",
    "                if overlap_query_lower <= domains[D][0] and overlap_query_upper >= domains[D][1]: # if yes\n",
    "                    median_shift_distance_lower = int(total_size / 2)\n",
    "                    median_shift_distance_upper = int(total_size / 2)\n",
    "                    min_median_distance = int(total_size / 2)\n",
    "                else: # if not\n",
    "                    median_shift_distance_lower = len(dataset[(dataset[:,D]>=overlap_query_lower) & (dataset[:,D] < median)])\n",
    "                    median_shift_distance_upper = len(dataset[(dataset[:,D]<=overlap_query_upper) & (dataset[:,D] > median)])\n",
    "                    min_median_distance = min(median_shift_distance_lower, median_shift_distance_upper)\n",
    "                    if median_shift_distance_lower < median_shift_distance_upper:\n",
    "                        split_value = overlap_query_lower\n",
    "                    else:\n",
    "                        split_value = overlap_query_upper\n",
    "\n",
    "            # record the split shift (i.e., min median distance) and split value\n",
    "            split_distance_each_dim.append(min_median_distance)\n",
    "            split_value_each_dim.append(split_value)\n",
    "            caches.append(cache)\n",
    "            medians.append(median)\n",
    "\n",
    "        # aftern calculating the min median distance for each dimension\n",
    "        split_distance_each_dim = np.asarray(split_distance_each_dim)\n",
    "        split_dimension = 0\n",
    "        split_value = 0\n",
    "        \n",
    "        \n",
    "        # Using Advanced Split\n",
    "        # if every dimension is not able to split!\n",
    "        if min(split_distance_each_dim) >= int((total_size / 2)-5):\n",
    "            for D in range(len(domains)):\n",
    "                query_trees[D].diveIn(caches[D][0], medians[D])\n",
    "            continue\n",
    "        \n",
    "        # degradation mechansim (if every dimension is valid to split, then using round robin)\n",
    "        # the 5 here is an error tolerance\n",
    "        if max(split_distance_each_dim) <= 5: # this should have the 2 sub partitions above the threshold size\n",
    "            split_dimension = current_dim + 1\n",
    "            if split_dimension >= len(domains):\n",
    "                split_dimension %= len(domains)\n",
    "            split_value = np.median(dataset[:,split_dimension])\n",
    "            break # jump to the split\n",
    "        \n",
    "        # the normal case\n",
    "        successful_split_flag = False\n",
    "        split_order = np.argsort(split_distance_each_dim)  # sort, ascending\n",
    "        for i in range(len(split_distance_each_dim)):\n",
    "            split_dimension = np.where(split_order==i)[0][0] # get the ith smallest\n",
    "            split_value = split_value_each_dim[split_dimension]\n",
    "            # if the partition is small and able to split, to avoid redundant reocrds that make it unsplitable by value\n",
    "            if total_size < 3*threshold and split_value_each_dim[split_dimension] == 0:\n",
    "                # split from median directly\n",
    "                split_from_median_position_tag = True\n",
    "                break\n",
    "            # check if the subnodes greater than threshold\n",
    "            sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "            sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "            if len(sub_dataset1) < threshold or len(sub_dataset2) < threshold:\n",
    "                continue \n",
    "            else:\n",
    "                successful_split_flag = True\n",
    "                break\n",
    "        \n",
    "        if split_from_median_position_tag:\n",
    "            break\n",
    "        \n",
    "        # Using Advanced Split if none of the above split can create legal sub partitions\n",
    "        if successful_split_flag:\n",
    "            break # jump to the split\n",
    "        else: # Using Advanced Split\n",
    "            dive_count = 0\n",
    "            for D in range(len(domains)):\n",
    "                if len(caches[D]) != 0:\n",
    "                    query_trees[D].diveIn(caches[D][0], medians[D])\n",
    "                    dive_count += 1\n",
    "            if dive_count == 0: # indicate none-of the split is OK\n",
    "                kdnodes = []\n",
    "                kdnodes.append([domains, total_size])\n",
    "                return kdnodes\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    sub_dataset1 = []\n",
    "    sub_dataset2 = []\n",
    "    if split_from_median_position_tag:\n",
    "        dataset = dataset[np.argsort((dataset[:,split_dimension]))]\n",
    "        sub_dataset1 = dataset[0:int(total_size/2)]\n",
    "        sub_dataset2 = dataset[int(total_size/2):-1]\n",
    "    else:\n",
    "        # split the dataset according to the split position\n",
    "        sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "        sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "\n",
    "    # change the domains\n",
    "    sub_domains1 = np.copy(domains)\n",
    "    sub_domains1[split_dimension][1] = split_value\n",
    "    sub_domains2 = np.copy(domains)\n",
    "    sub_domains2[split_dimension][0] = split_value\n",
    "\n",
    "    # filter the queries for each sub node\n",
    "    sub_query1 = query[query[:,split_dimension,0] < split_value]\n",
    "    sub_query2 = query[query[:,split_dimension,1] > split_value]\n",
    "    \n",
    "    # used to see the current depth\n",
    "    level += 1\n",
    "\n",
    "    # recursion\n",
    "    kdnodes = []\n",
    "    kdnodes.extend(LKDAdvanced2(sub_dataset1, sub_query1, sub_domains1, threshold, level, split_dimension))\n",
    "    kdnodes.extend(LKDAdvanced2(sub_dataset2, sub_query2, sub_domains2, threshold, level, split_dimension))\n",
    "    \n",
    "    return kdnodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
