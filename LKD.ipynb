{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Assistant Functions ===\n",
    "#\n",
    "# this works for one dimension only !!! An implementation of query bounding.\n",
    "\n",
    "def getoverlap(al, au, bl, bu):\n",
    "    return max(0, min(au,bu)-max(al,bl))\n",
    "\n",
    "# currently not used.\n",
    "def bounding_union(query_collection):\n",
    "    \n",
    "    # should keep it ordered first by the lower interval !!!!!!\n",
    "    query_collection = query_collection[query_collection[:,0].argsort()]\n",
    "    \n",
    "    remaining_query = query_collection\n",
    "    bounded_intervals = []\n",
    "    \n",
    "    while len(remaining_query) != 0:\n",
    "        \n",
    "        initial_interval = [remaining_query[0][0], remaining_query[0][1]]\n",
    "        temp_interval = []\n",
    "        \n",
    "        for i in range(len(remaining_query)-1):\n",
    "            \n",
    "            overlap = getoverlap(initial_interval[0],initial_interval[1],remaining_query[i+1][0], remaining_query[i+1][1])\n",
    "            \n",
    "            # there is no overlap\n",
    "            if overlap == 0:\n",
    "                temp_interval.append([remaining_query[i+1][0], remaining_query[i+1][1]])\n",
    "            else: # update interval border\n",
    "                initial_interval[0] = min(initial_interval[0], remaining_query[i+1][0])\n",
    "                initial_interval[1] = max(initial_interval[1], remaining_query[i+1][1])\n",
    "                \n",
    "        bounded_intervals.append(initial_interval)\n",
    "        remaining_query = temp_interval\n",
    "    \n",
    "    return bounded_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# === Learned KD-Tree Split (Fast Version) ===\n",
    "#\n",
    "# asssumption: the query boundings will not overlap. divide the KD-Tree recursively\n",
    "#\n",
    "# @dataset: contains the data only in this subnode, ordered in original load order;\n",
    "# @query: contains all the queries (bounded or original); ordered in original generated order;\n",
    "# @domains: the current domain of the node of every dimension [first lower, second upper],[]...; array object\n",
    "# @threshold: maximum page size\n",
    "# @level: the current tree depth\n",
    "#\n",
    "# return @kdnodes: contains the domain of each node and the correpsonding records amount, notice the domain is\n",
    "# ordered by the original load order as dataset\n",
    "def FastLearnedResuriveDivide(dataset, query, domains, threshold, level, current_dim = 0):\n",
    "\n",
    "    # check if the threshold is already satisfied\n",
    "    total_size = len(dataset)\n",
    "    #print('level: ', level, ' size: ', total_size, ' domain: ',domains)\n",
    "    if total_size <= threshold:\n",
    "        kdnodes = []\n",
    "        kdnodes.append([domains, total_size])\n",
    "        return kdnodes\n",
    "    \n",
    "    split_distance_each_dim = []\n",
    "    split_position_each_dim = []\n",
    "    split_value_each_dim = []\n",
    "    \n",
    "    # for each dimension, we calculated the distance from median to its first non-cross split\n",
    "    for D in range(len(dataset[0])):\n",
    "\n",
    "        # median, with fast median algorithm\n",
    "        median = np.median(dataset[:,D])\n",
    "        median_low = domains[D][0]\n",
    "        median_up = domains[D][1]    \n",
    "\n",
    "        # split position\n",
    "        split_distance = 0\n",
    "        split_position = int(total_size / 2)\n",
    "\n",
    "        # that is the place where we need query bounding !\n",
    "        if len(query) == 0:\n",
    "            split_distance_each_dim.append(split_distance)\n",
    "            split_position_each_dim.append(split_position)\n",
    "            split_value_each_dim.append(median)\n",
    "            continue\n",
    "\n",
    "        query = np.asarray(query)\n",
    "        query_in_this_dim = query[:, D]\n",
    "        \n",
    "        # bound the projected queries in this dimension\n",
    "        query_bound = bounding_union(query_in_this_dim)\n",
    "        \n",
    "        # check if the split position intersect some query boundings in this dim\n",
    "        for i in range(len(query_bound)):\n",
    "\n",
    "            # if intersect some query bounds (only possible to intersect one bounded query)\n",
    "            if median > query_bound[i][0] and median < query_bound[i][1]:\n",
    "\n",
    "                # check if the two end already exceeds domain, if yes, split from the middle\n",
    "                if query_bound[i][0] <= domains[D][0] and query_bound[i][1] >= domains[D][1]:\n",
    "                    split_distance = int(total_size / 2)\n",
    "                    break;\n",
    "                # if not exceeds, determine which side is closer to the median\n",
    "                else:\n",
    "                    # for the left side\n",
    "                    if query_bound[i][0] > domains[D][0]:\n",
    "                        median_low = query_bound[i][0]         \n",
    "                    # for the right side\n",
    "                    if query_bound[i][1] < domains[D][1]:\n",
    "                        median_up = query_bound[i][1]\n",
    "\n",
    "                # if not exceeds then choose the one that is closest from the median (in terms of #records!)\n",
    "                number_of_records_from_low_to_median = len(dataset[(dataset[:,D]>=median_low) & (dataset[:,D] < median)])\n",
    "                number_of_records_from_up_to_median = len(dataset[(dataset[:,D]<=median_up) & (dataset[:,D] > median)])\n",
    "                \n",
    "                if number_of_records_from_low_to_median <= number_of_records_from_up_to_median:\n",
    "                    median = median_low\n",
    "                    split_distance = number_of_records_from_low_to_median\n",
    "                else:\n",
    "                    median = median_up\n",
    "                    split_distance = number_of_records_from_up_to_median\n",
    "                    \n",
    "        # for each dimension, record its result\n",
    "        split_distance_each_dim.append(split_distance)\n",
    "        split_value_each_dim.append(median)\n",
    "\n",
    "    # aftern calculating the distance from median to its first non-cross split\n",
    "    split_distance_each_dim = np.asarray(split_distance_each_dim)\n",
    "    split_dimension = 0\n",
    "    split_value = 0\n",
    "    \n",
    "    # degradation mechansim (if no valid split position, then using round robin)\n",
    "    # if the median do not cross any historical query, split round robin to enhance robustness\n",
    "    if min(split_distance_each_dim) >= int((total_size / 2)-10) or max(split_distance_each_dim) <= 10:\n",
    "        split_dimension = current_dim + 1\n",
    "        if split_dimension >= len(domains):\n",
    "            split_dimension %= len(domains)\n",
    "        split_value = np.median(dataset[:,split_dimension])\n",
    "    else:\n",
    "        split_dimension = np.argmin(split_distance_each_dim)  # get the split dimension\n",
    "        split_value = split_value_each_dim[split_dimension]\n",
    "\n",
    "    # split the dataset according to the split position\n",
    "    sub_dataset1 = dataset[dataset[:,split_dimension] <= split_value]\n",
    "    sub_dataset2 = dataset[dataset[:,split_dimension] > split_value]\n",
    "    \n",
    "    if len(sub_dataset1) < threshold or len(sub_dataset2) < threshold:\n",
    "        kdnodes = []\n",
    "        kdnodes.append([domains, total_size])\n",
    "        return kdnodes\n",
    "    \n",
    "    # change the domains\n",
    "    sub_domains1 = np.copy(domains)\n",
    "    sub_domains1[split_dimension][1] = split_value\n",
    "    sub_domains2 = np.copy(domains)\n",
    "    sub_domains2[split_dimension][0] = split_value\n",
    "\n",
    "    # filter the queries for each sub node\n",
    "    sub_query1 = query[query[:,split_dimension,0] < split_value]\n",
    "    sub_query2 = query[query[:,split_dimension,1] > split_value]\n",
    "\n",
    "    # used to see the current depth\n",
    "    level += 1\n",
    "\n",
    "    # recursion\n",
    "    kdnodes = []\n",
    "    kdnodes.extend(FastLearnedResuriveDivide(sub_dataset1, sub_query1, sub_domains1, threshold, level, split_dimension))\n",
    "    kdnodes.extend(FastLearnedResuriveDivide(sub_dataset2, sub_query2, sub_domains2, threshold, level, split_dimension))\n",
    "\n",
    "    return kdnodes\n",
    "\n",
    "# start_time = time.time()\n",
    "# fast_kdnodes = FastLearnedResuriveDivide(dataset, training_set, domains_, block_size, 0)\n",
    "# end_time = time.time()\n",
    "# print(\"training time for fast Learned KD-Tree(s): \", end_time-start_time)\n",
    "# print('fast learned KD-Tree leaf nodes: ',len(fast_kdnodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
